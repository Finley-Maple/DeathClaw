{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explore participant data\n",
    "\n",
    "> This notebook explains how to explore phenotypic data table retrieve fields\n",
    "\n",
    "- runtime: 10min \n",
    "- recommended instance: mem1_ssd1_v2_x8\n",
    "- cost: <£0.10\n",
    "\n",
    "This notebook depends on:\n",
    "* **A Spark instance**\n",
    "\n",
    "In this notebook, we will dive deeper into the phenotypic data stored in the Spark database.\n",
    "We will retrieve the information about the fields, and learn how to get field id, title, and link to the UK Biobank Showcase, which provides more details and basic statistics about field."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import `dxdata` package and initialize Spark engine\n",
    "### Docs at: https://github.com/dnanexus/OpenBio/blob/master/dxdata/getting_started_with_dxdata.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import dxdata\n",
    "import os\n",
    "\n",
    "# Initialize dxdata engine\n",
    "engine = dxdata.connect(dialect=\"hive+pyspark\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connect to the dataset\n",
    "\n",
    "Next, we can set a `DATASET_ID` variable, which takes a value: `[projectID]:[dataset ID]`\n",
    "We use it to define the `dataset` with `dxdata.load_dataset` function.\n",
    "\n",
    "**projectID** and **dataset ID** values are unique to your project.\n",
    "Notebook example **101** explains how to get them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project = os.getenv('DX_PROJECT_CONTEXT_ID')\n",
    "record = os.popen(\"dx find data --type Dataset --delimiter ',' | awk -F ',' '{print $5}'\").read().rstrip()\n",
    "DATASET_ID = project + \":\" + record\n",
    "dataset = dxdata.load_dataset(id=DATASET_ID)\n",
    "\n",
    "pheno = dataset['participant']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract field names from ukb_field_mapping.json\n",
    "\n",
    "Now we'll load the field mapping JSON file and extract all field names in the format required for data extraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Load the field mapping JSON file\n",
    "# Update this path to match your actual file location\n",
    "mapping_file = 'DNAnexus:ukb_field_mapping.json'\n",
    "\n",
    "with open(mapping_file, 'r') as f:\n",
    "    field_mapping = json.load(f)\n",
    "\n",
    "# Extract all field IDs and convert to the format used in UK Biobank (prefixed with 'p')\n",
    "field_names = []\n",
    "for category, fields in field_mapping.items():\n",
    "    for field_id, description in fields.items():\n",
    "        field_name = f'p{field_id}'\n",
    "        field_names.append(field_name)\n",
    "\n",
    "# Remove duplicates and sort\n",
    "field_names = sorted(list(set(field_names)))\n",
    "\n",
    "print(f\"Total unique fields to extract: {len(field_names)}\")\n",
    "print(\"\\nField names:\")\n",
    "for field in field_names:\n",
    "    print(field)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verify fields exist in the dataset and get their titles\n",
    "\n",
    "Let's verify that these fields exist in the UK Biobank dataset and retrieve their full titles.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify fields and get their information (including ALL instances)\n",
    "# Use fuzzy matching to find all instances of each field (e.g., p50, p50_i0, p50_i1, etc.)\n",
    "\n",
    "verified_fields = []\n",
    "missing_fields = []\n",
    "\n",
    "for field_name in field_names:\n",
    "    try:\n",
    "        # Try exact match first (for fields without instances)\n",
    "        try:\n",
    "            field = pheno.find_field(name=field_name)\n",
    "            verified_fields.append({\n",
    "                'name': field.name,\n",
    "                'title': field.title,\n",
    "                'linkout': field.linkout\n",
    "            })\n",
    "            print(f\"✓ [{field.name}]\\t{field.title}\")\n",
    "        except:\n",
    "            # If exact match fails, search for all instances using regex\n",
    "            # Pattern matches: p50$ (exact) OR p50_i0, p50_i1, p50_i2, etc.\n",
    "            pattern = f\"^{field_name}$|^{field_name}_i\\\\d+$\"\n",
    "            instance_fields = list(pheno.find_fields(name_regex=pattern))\n",
    "            \n",
    "            if instance_fields:\n",
    "                for field in instance_fields:\n",
    "                    verified_fields.append({\n",
    "                        'name': field.name,\n",
    "                        'title': field.title,\n",
    "                        'linkout': field.linkout\n",
    "                    })\n",
    "                    print(f\"✓ [{field.name}]\\t{field.title}\")\n",
    "                print(f\"  → Found {len(instance_fields)} instances for {field_name}\")\n",
    "            else:\n",
    "                raise Exception(f\"No instances found\")\n",
    "                \n",
    "    except Exception as e:\n",
    "        missing_fields.append(field_name)\n",
    "        print(f\"✗ [{field_name}]\\tNot found: {str(e)}\")\n",
    "\n",
    "print(f\"\\n\\nSummary:\")\n",
    "print(f\"Found: {len(verified_fields)} fields (including all instances)\")\n",
    "print(f\"Missing: {len(missing_fields)} base fields\")\n",
    "if missing_fields:\n",
    "    print(f\"\\nMissing fields: {missing_fields}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save field names to JSON file\n",
    "\n",
    "Save the verified field names to a JSON file in the same format as the original field names.json file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save field names to JSON file\n",
    "output_file = '../biomni_logs_session_e1b5e60b-517b-48fb-a8df-486707b241bd_20251023_114557/extracted_field_names.json'\n",
    "\n",
    "# Save as list (one field per line like the original)\n",
    "with open(output_file, 'w') as f:\n",
    "    json.dump([f['name'] for f in verified_fields], f, indent=0)\n",
    "\n",
    "print(f\"Field names saved to: {output_file}\")\n",
    "print(f\"Total fields saved: {len(verified_fields)}\")\n",
    "\n",
    "# Also save detailed information\n",
    "detail_output_file = '../biomni_logs_session_e1b5e60b-517b-48fb-a8df-486707b241bd_20251023_114557/field_details.json'\n",
    "with open(detail_output_file, 'w') as f:\n",
    "    json.dump(verified_fields, f, indent=2)\n",
    "\n",
    "print(f\"Field details saved to: {detail_output_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract data for respiratory disease cohort\n",
    "\n",
    "This cell demonstrates how to extract the actual data for these fields. This may take some time depending on the dataset size.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get field objects for extraction\n",
    "field_objects = []\n",
    "for field_info in verified_fields:\n",
    "    try:\n",
    "        field = pheno.find_field(name=field_info['name'])\n",
    "        field_objects.append(field)\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading field {field_info['name']}: {e}\")\n",
    "\n",
    "# Add participant ID\n",
    "field_objects.insert(0, pheno.find_field(name=\"eid\"))\n",
    "\n",
    "# Ensure we have ICD-10 diagnosis fields for filtering\n",
    "# These fields contain the diagnosis codes\n",
    "icd10_fields = ['p41202', 'p41204', 'p41270']\n",
    "for icd_field in icd10_fields:\n",
    "    try:\n",
    "        # Check if already in field_objects\n",
    "        if not any(f.name == icd_field for f in field_objects):\n",
    "            field = pheno.find_field(name=icd_field)\n",
    "            field_objects.append(field)\n",
    "            print(f\"Added ICD-10 diagnosis field: {icd_field}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Could not add {icd_field}: {e}\")\n",
    "\n",
    "print(f\"\\nExtracting data for {len(field_objects)} fields...\")\n",
    "\n",
    "# Retrieve data (returns PySpark DataFrame)\n",
    "spark_df = pheno.retrieve_fields(fields=field_objects, engine=engine)\n",
    "\n",
    "# Get row count using PySpark method\n",
    "row_count = spark_df.count()\n",
    "col_count = len(spark_df.columns)\n",
    "print(f\"Data extracted: {row_count} participants, {col_count} fields\")\n",
    "\n",
    "# Convert to pandas DataFrame for easier manipulation\n",
    "print(\"\\nConverting to pandas DataFrame...\")\n",
    "df = spark_df.toPandas()\n",
    "\n",
    "# Now you can use pandas methods\n",
    "print(f\"Full dataset shape: {df.shape}\")\n",
    "print(\"\\nFirst few rows:\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Convert to Natural Language Text\n",
    "\n",
    "Now that we have extracted and filtered the respiratory cohort, we can convert the structured data to natural language text using the `process_respiratory_cohort.py` script.\n",
    "\n",
    "This step will:\n",
    "1. Read the CSV file with respiratory patients\n",
    "2. Convert each patient's structured data to narrative clinical text\n",
    "3. Save individual patient text files and a summary JSON\n",
    "\n",
    "The natural text output is ready for use with large language models and multimodal foundation models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 1: Import and use the converter directly in the notebook\n",
    "import sys\n",
    "sys.path.append('.')\n",
    "\n",
    "from process_respiratory_cohort import RespiratoryPatientTextConverter\n",
    "\n",
    "# Initialize converter\n",
    "converter = RespiratoryPatientTextConverter(\n",
    "    input_csv='ukb_respiratory_cohort.csv',\n",
    "    output_dir='./processed_respiratory_patients'\n",
    ")\n",
    "\n",
    "# Process first 10 patients as a test\n",
    "print(\"Processing respiratory cohort to natural language text...\")\n",
    "converter.process_cohort(max_patients=10)\n",
    "\n",
    "print(\"\\nProcessing complete! Check the output directory for results.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alternative: Run as command-line script\n",
    "\n",
    "You can also run the processing script from the command line:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run for all patients\n",
    "python process_respiratory_cohort.py \\\n",
    "    --input-csv ukb_respiratory_cohort.csv \\\n",
    "    --output-dir ./processed_respiratory_patients\n",
    "\n",
    "# Or run for first 100 patients only\n",
    "python process_respiratory_cohort.py \\\n",
    "    --input-csv ukb_respiratory_cohort.csv \\\n",
    "    --output-dir ./processed_respiratory_patients \\\n",
    "    --max-patients 100\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View Sample Natural Language Output\n",
    "\n",
    "Let's look at an example of the converted natural language text:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    "# Find the first processed patient file\n",
    "processed_dir = './processed_respiratory_patients'\n",
    "if os.path.exists(processed_dir):\n",
    "    patient_files = glob.glob(os.path.join(processed_dir, 'patient_*.txt'))\n",
    "    \n",
    "    if patient_files:\n",
    "        # Read and display the first patient's natural text\n",
    "        with open(patient_files[0], 'r') as f:\n",
    "            sample_text = f.read()\n",
    "        \n",
    "        print(\"=\" * 80)\n",
    "        print(\"SAMPLE NATURAL LANGUAGE OUTPUT\")\n",
    "        print(\"=\" * 80)\n",
    "        print(sample_text)\n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(f\"\\nTotal patients processed: {len(patient_files)}\")\n",
    "        print(f\"Files saved to: {processed_dir}\")\n",
    "    else:\n",
    "        print(\"No patient files found. Run the processing cell above first.\")\n",
    "else:\n",
    "    print(f\"Output directory not found: {processed_dir}\")\n",
    "    print(\"Run the processing cell above first.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter for Respiratory Disease Cohort\n",
    "\n",
    "We will filter participants to include only those with respiratory disease diagnoses based on ICD-10 codes:\n",
    "- **J09-J98**: Diseases of the respiratory system\n",
    "- **I26-I27**: Pulmonary heart disease and diseases of pulmonary circulation\n",
    "\n",
    "This filtering will be applied after data extraction using the diagnosis fields (p41202, p41204, p41270).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def has_respiratory_diagnosis(row):\n",
    "    \"\"\"\n",
    "    Check if a participant has respiratory disease diagnosis based on ICD-10 codes:\n",
    "    - J09-J98: Diseases of the respiratory system\n",
    "    - I26-I27: Pulmonary heart disease and diseases of pulmonary circulation\n",
    "    \n",
    "    Note: ICD-10 codes may be stored without decimal points (e.g., J181 = J18.1)\n",
    "    \"\"\"\n",
    "    # Collect all diagnosis codes from available ICD-10 fields\n",
    "    all_diagnoses = []\n",
    "    \n",
    "    for field in ['p41202', 'p41204', 'p41270']:\n",
    "        if field in row.index and row[field] is not None:\n",
    "            # Handle both list and single string values\n",
    "            if isinstance(row[field], list):\n",
    "                all_diagnoses.extend(row[field])\n",
    "            else:\n",
    "                all_diagnoses.append(str(row[field]))\n",
    "    \n",
    "    # Check each diagnosis code\n",
    "    for code in all_diagnoses:\n",
    "        if code and isinstance(code, str):\n",
    "            # Remove any whitespace and convert to uppercase\n",
    "            code = code.strip().upper()\n",
    "            \n",
    "            # Check for J09-J98 (respiratory diseases)\n",
    "            # ICD-10 format: Letter + 2-3 digits (category) + optional subcategory\n",
    "            # Examples: J18.1 (stored as J181), J96.90 (stored as J9690)\n",
    "            if code.startswith('J'):\n",
    "                # Extract the category code (first 2 digits after 'J')\n",
    "                # For J codes, category is always 2 digits: J00-J99\n",
    "                match = re.match(r'J(\\d{2})', code)\n",
    "                if match:\n",
    "                    category = int(match.group(1))\n",
    "                    if 9 <= category <= 98:\n",
    "                        return True\n",
    "            \n",
    "            # Check for I26-I27 (pulmonary heart disease)\n",
    "            # For I codes, category is also 2 digits: I00-I99\n",
    "            elif code.startswith('I'):\n",
    "                match = re.match(r'I(\\d{2})', code)\n",
    "                if match:\n",
    "                    category = int(match.group(1))\n",
    "                    if 26 <= category <= 27:\n",
    "                        return True\n",
    "    \n",
    "    return False\n",
    "\n",
    "# Apply filter to get respiratory disease cohort\n",
    "print(\"Filtering for respiratory disease patients...\")\n",
    "df['has_respiratory_disease'] = df.apply(has_respiratory_diagnosis, axis=1)\n",
    "\n",
    "respiratory_df = df[df['has_respiratory_disease'] == True].copy()\n",
    "respiratory_df = respiratory_df.drop(columns=['has_respiratory_disease'])\n",
    "\n",
    "print(f\"\\nCohort filtering results:\")\n",
    "print(f\"Original dataset: {len(df)} participants\")\n",
    "print(f\"Respiratory disease cohort: {len(respiratory_df)} participants ({len(respiratory_df)/len(df)*100:.2f}%)\")\n",
    "\n",
    "# Show some diagnosis examples\n",
    "print(\"\\nExample diagnoses from filtered cohort (first 5 participants):\")\n",
    "for idx, row in respiratory_df.head().iterrows():\n",
    "    diagnoses = []\n",
    "    for field in ['p41202', 'p41204', 'p41270']:\n",
    "        if field in row.index and row[field] is not None:\n",
    "            if isinstance(row[field], list):\n",
    "                diagnoses.extend(row[field])\n",
    "            else:\n",
    "                diagnoses.append(str(row[field]))\n",
    "    \n",
    "    # Filter for respiratory codes only using correct parsing\n",
    "    resp_codes = []\n",
    "    for d in diagnoses:\n",
    "        if d and isinstance(d, str):\n",
    "            d_clean = d.strip().upper()\n",
    "            # Check J09-J98\n",
    "            if d_clean.startswith('J'):\n",
    "                match = re.match(r'J(\\d{2})', d_clean)\n",
    "                if match and 9 <= int(match.group(1)) <= 98:\n",
    "                    resp_codes.append(d)\n",
    "            # Check I26-I27\n",
    "            elif d_clean.startswith('I'):\n",
    "                match = re.match(r'I(\\d{2})', d_clean)\n",
    "                if match and 26 <= int(match.group(1)) <= 27:\n",
    "                    resp_codes.append(d)\n",
    "    \n",
    "    print(f\"  Participant {row['eid']}: {', '.join(resp_codes[:10])}\")  # Show first 10 codes\n",
    "\n",
    "# Save filtered cohort to CSV\n",
    "output_csv = 'ukb_respiratory_cohort.csv'\n",
    "respiratory_df.to_csv(output_csv, index=False)\n",
    "print(f\"\\nFiltered respiratory cohort saved to: {output_csv}\")\n",
    "\n",
    "# Also save full dataset (unfiltered) for comparison\n",
    "output_csv_full = 'ukb_full_data.csv'\n",
    "df.to_csv(output_csv_full, index=False)\n",
    "print(f\"Full dataset saved to: {output_csv_full}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze ICD-10 Code Distribution in Respiratory Cohort\n",
    "\n",
    "Let's examine the distribution of respiratory ICD-10 codes to understand the types of respiratory diseases in our cohort.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Collect all respiratory ICD-10 codes from the cohort\n",
    "all_resp_codes = []\n",
    "\n",
    "for idx, row in respiratory_df.iterrows():\n",
    "    for field in ['p41202', 'p41204', 'p41270']:\n",
    "        if field in row.index and row[field] is not None:\n",
    "            if isinstance(row[field], list):\n",
    "                codes = row[field]\n",
    "            else:\n",
    "                codes = [str(row[field])]\n",
    "            \n",
    "            # Filter for respiratory codes (J09-J98, I26-I27)\n",
    "            for code in codes:\n",
    "                if code and isinstance(code, str):\n",
    "                    code = code.strip().upper()\n",
    "                    # Check if it's a respiratory code\n",
    "                    # Use correct ICD-10 parsing: extract first 2 digits (category)\n",
    "                    if code.startswith('J'):\n",
    "                        match = re.match(r'J(\\d{2})', code)\n",
    "                        if match:\n",
    "                            category = int(match.group(1))\n",
    "                            if 9 <= category <= 98:\n",
    "                                all_resp_codes.append(code)\n",
    "                    elif code.startswith('I'):\n",
    "                        match = re.match(r'I(\\d{2})', code)\n",
    "                        if match:\n",
    "                            category = int(match.group(1))\n",
    "                            if 26 <= category <= 27:\n",
    "                                all_resp_codes.append(code)\n",
    "\n",
    "# Count frequency of each code\n",
    "code_counts = Counter(all_resp_codes)\n",
    "\n",
    "print(f\"Total respiratory diagnoses recorded: {len(all_resp_codes)}\")\n",
    "print(f\"Unique respiratory diagnosis codes: {len(code_counts)}\")\n",
    "print(f\"\\nTop 20 most common respiratory diagnoses:\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for code, count in code_counts.most_common(20):\n",
    "    percentage = (count / len(respiratory_df)) * 100\n",
    "    print(f\"{code:8s} : {count:6d} cases ({percentage:5.2f}% of cohort)\")\n",
    "\n",
    "# Breakdown by major category\n",
    "j_codes = {k: v for k, v in code_counts.items() if k.startswith('J')}\n",
    "i_codes = {k: v for k, v in code_counts.items() if k.startswith('I')}\n",
    "\n",
    "print(f\"\\n\\nBreakdown by ICD-10 category:\")\n",
    "print(f\"J codes (Respiratory system diseases): {sum(j_codes.values())} diagnoses across {len(j_codes)} unique codes\")\n",
    "print(f\"I26-I27 codes (Pulmonary heart disease): {sum(i_codes.values())} diagnoses across {len(i_codes)} unique codes\")\n",
    "\n",
    "# Save code statistics to file\n",
    "import json\n",
    "code_stats = {\n",
    "    'total_participants': len(respiratory_df),\n",
    "    'total_diagnoses': len(all_resp_codes),\n",
    "    'unique_codes': len(code_counts),\n",
    "    'top_20_codes': [{'code': code, 'count': count, 'percentage': round((count/len(respiratory_df))*100, 2)} \n",
    "                     for code, count in code_counts.most_common(20)],\n",
    "    'category_breakdown': {\n",
    "        'J_codes': {'count': sum(j_codes.values()), 'unique': len(j_codes)},\n",
    "        'I_codes': {'count': sum(i_codes.values()), 'unique': len(i_codes)}\n",
    "    }\n",
    "}\n",
    "\n",
    "stats_file = 'icd10_code_statistics.json'\n",
    "with open(stats_file, 'w') as f:\n",
    "    json.dump(code_stats, f, indent=2)\n",
    "    \n",
    "print(f\"\\n\\nCode statistics saved to: {stats_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieve data from the table\n",
    "\n",
    "The following code selects the `participant` table.\n",
    "Then we can define which field we are interested in using the `find_field` function.\n",
    "\n",
    "There are three main ways to identify the field of interest:\n",
    "\n",
    "- With `name` argument: here we give field ID. We can construct field ID used by the `dxdata` package from the field ID defined by UKB Showcase. The numeric showcase ID is translated to the Spark DB column name by adding the letter `p` at the beginning: e.g. *Standing height* showcase id is `50`, so Spark ID would be `p50`. Usually, fields have multiple instances. In such case, we add the `_i` suffix followed by instance number, e.g. *Standing height | Instance 0* will be `p50_i0`\n",
    "- With `title` argument: here we define the field by full title, followed by ` | Instance` suffix, e.g. `Age at recruitment` or `Standing height | Instance 0`\n",
    "- With `title_regex` argument: here we define the field by [regular expression](https://docs.python.org/3/howto/regex.html) matching the part of the title. We can use a keyword here, e.g. `.*height.*` will return all columns with the word *height* in the title."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pheno = dataset['participant']\n",
    "\n",
    "# Find by field name\n",
    "field_eid = pheno.find_field(name=\"eid\")\n",
    "\n",
    "# Find by exact title\n",
    "field_sex = pheno.find_field(title=\"Sex\")\n",
    "field_age = pheno.find_field(title=\"Age at recruitment\")\n",
    "field_height = pheno.find_field(title=\"Standing height | Instance 0\")\n",
    "\n",
    "# Find by title pattern\n",
    "pattern = \".*height.*\"\n",
    "fields_height = list(pheno.find_fields(title_regex=pattern))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[p50_i0]\tStanding height | Instance 0 (http://biobank.ctsu.ox.ac.uk/crystal/field.cgi?id=50)\n",
      "[p50_i1]\tStanding height | Instance 1 (http://biobank.ctsu.ox.ac.uk/crystal/field.cgi?id=50)\n",
      "[p50_i2]\tStanding height | Instance 2 (http://biobank.ctsu.ox.ac.uk/crystal/field.cgi?id=50)\n",
      "[p50_i3]\tStanding height | Instance 3 (http://biobank.ctsu.ox.ac.uk/crystal/field.cgi?id=50)\n",
      "[p51_i0]\tSeated height | Instance 0 (http://biobank.ctsu.ox.ac.uk/crystal/field.cgi?id=51)\n",
      "[p51_i1]\tSeated height | Instance 1 (http://biobank.ctsu.ox.ac.uk/crystal/field.cgi?id=51)\n",
      "[p51_i2]\tSeated height | Instance 2 (http://biobank.ctsu.ox.ac.uk/crystal/field.cgi?id=51)\n",
      "[p51_i3]\tSeated height | Instance 3 (http://biobank.ctsu.ox.ac.uk/crystal/field.cgi?id=51)\n",
      "[p1697_i0]\tComparative height size at age 10 | Instance 0 (http://biobank.ctsu.ox.ac.uk/crystal/field.cgi?id=1697)\n",
      "[p1697_i1]\tComparative height size at age 10 | Instance 1 (http://biobank.ctsu.ox.ac.uk/crystal/field.cgi?id=1697)\n",
      "[p1697_i2]\tComparative height size at age 10 | Instance 2 (http://biobank.ctsu.ox.ac.uk/crystal/field.cgi?id=1697)\n",
      "[p3077_i0]\tSeating box height | Instance 0 (http://biobank.ctsu.ox.ac.uk/crystal/field.cgi?id=3077)\n",
      "[p3077_i1]\tSeating box height | Instance 1 (http://biobank.ctsu.ox.ac.uk/crystal/field.cgi?id=3077)\n",
      "[p3077_i2]\tSeating box height | Instance 2 (http://biobank.ctsu.ox.ac.uk/crystal/field.cgi?id=3077)\n",
      "[p3077_i3]\tSeating box height | Instance 3 (http://biobank.ctsu.ox.ac.uk/crystal/field.cgi?id=3077)\n",
      "[p20015_i0]\tSitting height | Instance 0 (http://biobank.ctsu.ox.ac.uk/crystal/field.cgi?id=20015)\n",
      "[p20015_i1]\tSitting height | Instance 1 (http://biobank.ctsu.ox.ac.uk/crystal/field.cgi?id=20015)\n",
      "[p20015_i2]\tSitting height | Instance 2 (http://biobank.ctsu.ox.ac.uk/crystal/field.cgi?id=20015)\n",
      "[p20015_i3]\tSitting height | Instance 3 (http://biobank.ctsu.ox.ac.uk/crystal/field.cgi?id=20015)\n",
      "[p20047_i0]\tReason for skipping standing height | Instance 0 (http://biobank.ctsu.ox.ac.uk/crystal/field.cgi?id=20047)\n",
      "[p20048_i0]\tReason for skipping sitting height | Instance 0 (http://biobank.ctsu.ox.ac.uk/crystal/field.cgi?id=20048)\n",
      "[p23201_i2]\tL1-L4 average height | Instance 2 (http://biobank.ctsu.ox.ac.uk/crystal/field.cgi?id=23201)\n",
      "[p23201_i3]\tL1-L4 average height | Instance 3 (http://biobank.ctsu.ox.ac.uk/crystal/field.cgi?id=23201)\n",
      "[p26240]\tStandard PRS for height (HEIGHT) (http://biobank.ctsu.ox.ac.uk/crystal/field.cgi?id=26240)\n",
      "[p26241]\tEnhanced PRS for height (HEIGHT) (http://biobank.ctsu.ox.ac.uk/crystal/field.cgi?id=26241)\n"
     ]
    }
   ],
   "source": [
    "for _ in fields_height:\n",
    "    print(\"[\" + _.name + \"]\\t\" + _.title + \" (\" + _.linkout + \")\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "be0617d24f23f3f0ff0f78cfff875dd0cc8ce9ddccca39efd47dcbfb80ba815b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
