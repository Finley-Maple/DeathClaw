{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import `dxdata` package and initialize Spark engine\n",
    "### Docs at: https://github.com/dnanexus/OpenBio/blob/master/dxdata/getting_started_with_dxdata.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import dxdata\n",
    "import os\n",
    "\n",
    "# Initialize dxdata engine\n",
    "engine = dxdata.connect(dialect=\"hive+pyspark\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connect to the dataset\n",
    "\n",
    "Next, we can set a `DATASET_ID` variable, which takes a value: `[projectID]:[dataset ID]`\n",
    "We use it to define the `dataset` with `dxdata.load_dataset` function.\n",
    "\n",
    "**projectID** and **dataset ID** values are unique to your project.\n",
    "Notebook example **101** explains how to get them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "project = os.getenv('DX_PROJECT_CONTEXT_ID')\n",
    "record = os.popen(\"dx find data --type Dataset --delimiter ',' | awk -F ',' '{print $5}'\").read().rstrip()\n",
    "DATASET_ID = project + \":\" + record\n",
    "dataset = dxdata.load_dataset(id=DATASET_ID)\n",
    "\n",
    "pheno = dataset['participant']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": [],
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'project-J3YgK2QJ50vb4pG71P1gk3gQ'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": [],
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'record-J3Yzq5QJ405q1YQKv2Bq1GZj\\n'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.popen(\"dx find data --type Dataset --delimiter ',' | awk -F ',' '{print $5}'\").read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract field names from ukb_field_mapping.json\n",
    "\n",
    "Now we'll load the field mapping JSON file and extract all field names in the format required for data extraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": [],
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " A102_Explore-participant-data_Python.ipynb    'basics field names.json'\n",
      " Bulk\t\t\t\t\t        data_extraction.ipynb\n",
      "'Respiratory patients'\t\t\t        data_participant.csv\n",
      "'Showcase metadata'\t\t\t       'field names.json'\n",
      "'Untitled Workflow - 10___15___2025 11:24 PM'   ukb_field_mapping.json\n",
      " app240523_20251009163455\t\t        ukb_respiratory_pipeline.py\n",
      " app240523_20251009163455.dataset\n"
     ]
    }
   ],
   "source": [
    "!ls /mnt/project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [],
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total unique fields to extract: 86\n",
      "\n",
      "Field names:\n",
      "p1558\n",
      "p1787\n",
      "p20002\n",
      "p20116\n",
      "p20126\n",
      "p20205\n",
      "p20208\n",
      "p20252\n",
      "p20255\n",
      "p20256\n",
      "p20257\n",
      "p20258\n",
      "p2090\n",
      "p2100\n",
      "p21000\n",
      "p21001\n",
      "p22506\n",
      "p23104\n",
      "p2316\n",
      "p2335\n",
      "p24003\n",
      "p24004\n",
      "p24005\n",
      "p24006\n",
      "p24016\n",
      "p2867\n",
      "p2887\n",
      "p30000\n",
      "p30010\n",
      "p30020\n",
      "p30030\n",
      "p30040\n",
      "p30050\n",
      "p30060\n",
      "p30070\n",
      "p30080\n",
      "p30090\n",
      "p30100\n",
      "p30110\n",
      "p30120\n",
      "p30130\n",
      "p30140\n",
      "p30150\n",
      "p30600\n",
      "p30610\n",
      "p30620\n",
      "p3063\n",
      "p30630\n",
      "p3064\n",
      "p30640\n",
      "p30650\n",
      "p30660\n",
      "p30670\n",
      "p30680\n",
      "p30690\n",
      "p30700\n",
      "p30710\n",
      "p30720\n",
      "p30730\n",
      "p30740\n",
      "p30750\n",
      "p30760\n",
      "p30770\n",
      "p30780\n",
      "p30790\n",
      "p30800\n",
      "p30810\n",
      "p31\n",
      "p34\n",
      "p3786\n",
      "p40001\n",
      "p40002\n",
      "p41202\n",
      "p41204\n",
      "p41270\n",
      "p4559\n",
      "p4598\n",
      "p4631\n",
      "p4717\n",
      "p50\n",
      "p52\n",
      "p5375\n",
      "p5663\n",
      "p6138\n",
      "p6152\n",
      "p738\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Load the hierarchical field mapping JSON file\n",
    "# Update this path to match your actual file location\n",
    "mapping_file = '/mnt/project/ukb_field_mapping_new.json'\n",
    "\n",
    "with open(mapping_file, 'r') as f:\n",
    "    field_mapping = json.load(f)\n",
    "\n",
    "def extract_field_ids_recursive(data, field_ids=None, category_name=None):\n",
    "    \"\"\"\n",
    "    Recursively extract field IDs from the hierarchical JSON structure.\n",
    "    \n",
    "    A field entry is identified by having 'name' and 'value_type' keys.\n",
    "    Category levels are dictionaries that don't have these keys.\n",
    "    \n",
    "    Args:\n",
    "        data: Dictionary (can be nested categories or field entries)\n",
    "        field_ids: List to accumulate field IDs\n",
    "    \n",
    "    Returns:\n",
    "        List of field IDs\n",
    "    \"\"\"\n",
    "    if field_ids is None:\n",
    "        field_ids = []\n",
    "    \n",
    "    if isinstance(data, dict):\n",
    "        # Check if this is a field entry (has 'name' and 'value_type')\n",
    "        if 'name' in data and 'value_type' in data:\n",
    "            # This is a field entry, not a category - skip it\n",
    "            return field_ids\n",
    "        \n",
    "        # Otherwise, iterate through the dictionary\n",
    "        for key, value in data.items():\n",
    "            if isinstance(value, dict):\n",
    "                # Check if this value is a field entry\n",
    "                if 'name' in value and 'value_type' in value:\n",
    "                    # This key is a field ID\n",
    "                    field_ids.append(key)\n",
    "                else:\n",
    "                    # This is a nested category, recurse into it\n",
    "                    extract_field_ids_recursive(value, field_ids)\n",
    "    \n",
    "    return field_ids\n",
    "\n",
    "def get_category_fields(field_mapping, category_path=None):\n",
    "    \"\"\"\n",
    "    Extract field IDs from either all categories or a specific category.\n",
    "    \n",
    "    Args:\n",
    "        field_mapping: The complete hierarchical field mapping dictionary\n",
    "        category_path: Optional list of category names to navigate to specific category\n",
    "                      e.g., ['Lifestyle and environment', 'Smoking'] \n",
    "                      If None, extracts all fields from entire structure\n",
    "    \n",
    "    Returns:\n",
    "        List of field names (prefixed with 'p')\n",
    "    \"\"\"\n",
    "    # If no category path specified, extract all fields\n",
    "    if category_path is None:\n",
    "        field_ids = extract_field_ids_recursive(field_mapping)\n",
    "    else:\n",
    "        # Navigate to the specific category\n",
    "        current_level = field_mapping\n",
    "        \n",
    "        for category in category_path:\n",
    "            if category in current_level:\n",
    "                current_level = current_level[category]\n",
    "            else:\n",
    "                print(f\"Warning: Category '{category}' not found in the structure\")\n",
    "                print(f\"Available categories at this level: {list(current_level.keys())}\")\n",
    "                return []\n",
    "        \n",
    "        # Extract fields only from this category\n",
    "        field_ids = extract_field_ids_recursive(current_level)\n",
    "    \n",
    "    # Convert to the format used in UK Biobank (prefixed with 'p')\n",
    "    field_names = [f'p{field_id}' for field_id in field_ids]\n",
    "    \n",
    "    # Remove duplicates and sort\n",
    "    field_names = sorted(list(set(field_names)))\n",
    "    \n",
    "    return field_names\n",
    "\n",
    "def print_categories(field_mapping, indent=0):\n",
    "    \"\"\"\n",
    "    Helper function to print the category structure for easy navigation.\n",
    "    \n",
    "    Args:\n",
    "        field_mapping: The hierarchical field mapping dictionary\n",
    "        indent: Current indentation level (for display)\n",
    "    \"\"\"\n",
    "    if isinstance(field_mapping, dict):\n",
    "        # Skip if this is a field entry\n",
    "        if 'name' in field_mapping and 'value_type' in field_mapping:\n",
    "            return\n",
    "        \n",
    "        for key, value in field_mapping.items():\n",
    "            if isinstance(value, dict) and not ('name' in value and 'value_type' in value):\n",
    "                # This is a category\n",
    "                print(\"  \" * indent + f\"- {key}\")\n",
    "                print_categories(value, indent + 1)\n",
    "\n",
    "# OPTION 1: Extract ALL fields (default behavior)\n",
    "print(\"=\" * 70)\n",
    "print(\"EXTRACTING ALL FIELDS FROM ALL CATEGORIES\")\n",
    "print(\"=\" * 70)\n",
    "field_names = get_category_fields(field_mapping, category_path=None)\n",
    "\n",
    "print(f\"Total unique fields to extract: {len(field_names)}\")\n",
    "print(f\"\\nFirst 10 field names:\")\n",
    "for field in field_names[:10]:\n",
    "    print(f\"  {field}\")\n",
    "\n",
    "# OPTION 2: Extract fields from a SPECIFIC CATEGORY\n",
    "# Uncomment and modify the category_path below to extract only one category\n",
    "\n",
    "# print(\"\\n\" + \"=\" * 70)\n",
    "# print(\"AVAILABLE CATEGORIES\")\n",
    "# print(\"=\" * 70)\n",
    "# print_categories(field_mapping)\n",
    "# \n",
    "# print(\"\\n\" + \"=\" * 70)\n",
    "# print(\"EXTRACTING FIELDS FROM SPECIFIC CATEGORY\")\n",
    "# print(\"=\" * 70)\n",
    "# # Example: Extract only respiratory-related fields\n",
    "# # Modify the category_path list to navigate to your desired category\n",
    "# category_path = ['Health and medical history', 'Respiratory']  # Adjust as needed\n",
    "# field_names = get_category_fields(field_mapping, category_path=category_path)\n",
    "# \n",
    "# print(f\"Category: {' -> '.join(category_path)}\")\n",
    "# print(f\"Total unique fields to extract: {len(field_names)}\")\n",
    "# print(f\"\\nField names:\")\n",
    "# for field in field_names:\n",
    "#     print(f\"  {field}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "tags": [],
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[p3063_i0_a0]\tForced expiratory volume in 1-second (FEV1) | Instance 0 | Array 0 (http://biobank.ctsu.ox.ac.uk/crystal/field.cgi?id=3063)\n",
      "[p3063_i0_a1]\tForced expiratory volume in 1-second (FEV1) | Instance 0 | Array 1 (http://biobank.ctsu.ox.ac.uk/crystal/field.cgi?id=3063)\n",
      "[p3063_i0_a2]\tForced expiratory volume in 1-second (FEV1) | Instance 0 | Array 2 (http://biobank.ctsu.ox.ac.uk/crystal/field.cgi?id=3063)\n",
      "[p3063_i1_a0]\tForced expiratory volume in 1-second (FEV1) | Instance 1 | Array 0 (http://biobank.ctsu.ox.ac.uk/crystal/field.cgi?id=3063)\n",
      "[p3063_i1_a1]\tForced expiratory volume in 1-second (FEV1) | Instance 1 | Array 1 (http://biobank.ctsu.ox.ac.uk/crystal/field.cgi?id=3063)\n",
      "[p3063_i1_a2]\tForced expiratory volume in 1-second (FEV1) | Instance 1 | Array 2 (http://biobank.ctsu.ox.ac.uk/crystal/field.cgi?id=3063)\n",
      "[p3063_i2_a0]\tForced expiratory volume in 1-second (FEV1) | Instance 2 | Array 0 (http://biobank.ctsu.ox.ac.uk/crystal/field.cgi?id=3063)\n",
      "[p3063_i2_a1]\tForced expiratory volume in 1-second (FEV1) | Instance 2 | Array 1 (http://biobank.ctsu.ox.ac.uk/crystal/field.cgi?id=3063)\n",
      "[p3063_i2_a2]\tForced expiratory volume in 1-second (FEV1) | Instance 2 | Array 2 (http://biobank.ctsu.ox.ac.uk/crystal/field.cgi?id=3063)\n",
      "[p3063_i3_a0]\tForced expiratory volume in 1-second (FEV1) | Instance 3 | Array 0 (http://biobank.ctsu.ox.ac.uk/crystal/field.cgi?id=3063)\n",
      "[p3063_i3_a1]\tForced expiratory volume in 1-second (FEV1) | Instance 3 | Array 1 (http://biobank.ctsu.ox.ac.uk/crystal/field.cgi?id=3063)\n",
      "[p3063_i3_a2]\tForced expiratory volume in 1-second (FEV1) | Instance 3 | Array 2 (http://biobank.ctsu.ox.ac.uk/crystal/field.cgi?id=3063)\n",
      "[p10695_i0_a1]\tForced expiratory volume in 1-second (FEV1) (pilot) | Instance 0 | Array 1 (http://biobank.ctsu.ox.ac.uk/crystal/field.cgi?id=10695)\n",
      "[p10695_i0_a2]\tForced expiratory volume in 1-second (FEV1) (pilot) | Instance 0 | Array 2 (http://biobank.ctsu.ox.ac.uk/crystal/field.cgi?id=10695)\n",
      "[p10695_i0_a3]\tForced expiratory volume in 1-second (FEV1) (pilot) | Instance 0 | Array 3 (http://biobank.ctsu.ox.ac.uk/crystal/field.cgi?id=10695)\n",
      "[p10695_i0_a4]\tForced expiratory volume in 1-second (FEV1) (pilot) | Instance 0 | Array 4 (http://biobank.ctsu.ox.ac.uk/crystal/field.cgi?id=10695)\n",
      "[p20150_i0]\tForced expiratory volume in 1-second (FEV1), Best measure | Instance 0 (http://biobank.ctsu.ox.ac.uk/crystal/field.cgi?id=20150)\n",
      "[p20153_i0]\tForced expiratory volume in 1-second (FEV1), predicted | Instance 0 (http://biobank.ctsu.ox.ac.uk/crystal/field.cgi?id=20153)\n",
      "[p20154_i0]\tForced expiratory volume in 1-second (FEV1), predicted percentage | Instance 0 (http://biobank.ctsu.ox.ac.uk/crystal/field.cgi?id=20154)\n",
      "[p20256]\tForced expiratory volume in 1-second (FEV1) Z-score (http://biobank.ctsu.ox.ac.uk/crystal/field.cgi?id=20256)\n",
      "[p20258]\tFEV1/ FVC ratio Z-score (http://biobank.ctsu.ox.ac.uk/crystal/field.cgi?id=20258)\n"
     ]
    }
   ],
   "source": [
    "pattern = \".*FEV.*\"\n",
    "fields_height = list(pheno.find_fields(title_regex=pattern))\n",
    "\n",
    "for _ in fields_height:\n",
    "    print(\"[\" + _.name + \"]\\t\" + _.title + \" (\" + _.linkout + \")\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verify fields exist in the dataset and get their titles\n",
    "\n",
    "Let's verify that these fields exist in the UK Biobank dataset and retrieve their full titles.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "tags": [],
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ [p1558_i0]\tAlcohol intake frequency. | Instance 0\n",
      "✓ [p1558_i1]\tAlcohol intake frequency. | Instance 1\n",
      "✓ [p1558_i2]\tAlcohol intake frequency. | Instance 2\n",
      "✓ [p1558_i3]\tAlcohol intake frequency. | Instance 3\n",
      "  → Found 4 instances for p1558\n",
      "✓ [p1787_i0]\tMaternal smoking around birth | Instance 0\n",
      "✓ [p1787_i1]\tMaternal smoking around birth | Instance 1\n",
      "✓ [p1787_i2]\tMaternal smoking around birth | Instance 2\n",
      "  → Found 3 instances for p1787\n",
      "✓ [p20002_i0_a0]\tNon-cancer illness code, self-reported | Instance 0 | Array 0\n",
      "✓ [p20002_i0_a1]\tNon-cancer illness code, self-reported | Instance 0 | Array 1\n",
      "✓ [p20002_i0_a2]\tNon-cancer illness code, self-reported | Instance 0 | Array 2\n",
      "✓ [p20002_i0_a3]\tNon-cancer illness code, self-reported | Instance 0 | Array 3\n",
      "✓ [p20002_i0_a4]\tNon-cancer illness code, self-reported | Instance 0 | Array 4\n",
      "✓ [p20002_i0_a5]\tNon-cancer illness code, self-reported | Instance 0 | Array 5\n",
      "✓ [p20002_i0_a6]\tNon-cancer illness code, self-reported | Instance 0 | Array 6\n",
      "✓ [p20002_i0_a7]\tNon-cancer illness code, self-reported | Instance 0 | Array 7\n",
      "✓ [p20002_i0_a8]\tNon-cancer illness code, self-reported | Instance 0 | Array 8\n",
      "✓ [p20002_i0_a9]\tNon-cancer illness code, self-reported | Instance 0 | Array 9\n",
      "✓ [p20002_i0_a10]\tNon-cancer illness code, self-reported | Instance 0 | Array 10\n",
      "✓ [p20002_i0_a11]\tNon-cancer illness code, self-reported | Instance 0 | Array 11\n",
      "✓ [p20002_i0_a12]\tNon-cancer illness code, self-reported | Instance 0 | Array 12\n",
      "✓ [p20002_i0_a13]\tNon-cancer illness code, self-reported | Instance 0 | Array 13\n",
      "✓ [p20002_i0_a14]\tNon-cancer illness code, self-reported | Instance 0 | Array 14\n",
      "✓ [p20002_i0_a15]\tNon-cancer illness code, self-reported | Instance 0 | Array 15\n",
      "✓ [p20002_i0_a16]\tNon-cancer illness code, self-reported | Instance 0 | Array 16\n",
      "✓ [p20002_i0_a17]\tNon-cancer illness code, self-reported | Instance 0 | Array 17\n",
      "✓ [p20002_i0_a18]\tNon-cancer illness code, self-reported | Instance 0 | Array 18\n",
      "✓ [p20002_i0_a19]\tNon-cancer illness code, self-reported | Instance 0 | Array 19\n",
      "✓ [p20002_i0_a20]\tNon-cancer illness code, self-reported | Instance 0 | Array 20\n",
      "✓ [p20002_i0_a21]\tNon-cancer illness code, self-reported | Instance 0 | Array 21\n",
      "✓ [p20002_i0_a22]\tNon-cancer illness code, self-reported | Instance 0 | Array 22\n",
      "✓ [p20002_i0_a23]\tNon-cancer illness code, self-reported | Instance 0 | Array 23\n",
      "✓ [p20002_i0_a24]\tNon-cancer illness code, self-reported | Instance 0 | Array 24\n",
      "✓ [p20002_i0_a25]\tNon-cancer illness code, self-reported | Instance 0 | Array 25\n",
      "✓ [p20002_i0_a26]\tNon-cancer illness code, self-reported | Instance 0 | Array 26\n",
      "✓ [p20002_i0_a27]\tNon-cancer illness code, self-reported | Instance 0 | Array 27\n",
      "✓ [p20002_i0_a28]\tNon-cancer illness code, self-reported | Instance 0 | Array 28\n",
      "✓ [p20002_i0_a29]\tNon-cancer illness code, self-reported | Instance 0 | Array 29\n",
      "✓ [p20002_i0_a30]\tNon-cancer illness code, self-reported | Instance 0 | Array 30\n",
      "✓ [p20002_i0_a31]\tNon-cancer illness code, self-reported | Instance 0 | Array 31\n",
      "✓ [p20002_i0_a32]\tNon-cancer illness code, self-reported | Instance 0 | Array 32\n",
      "✓ [p20002_i0_a33]\tNon-cancer illness code, self-reported | Instance 0 | Array 33\n",
      "✓ [p20002_i1_a0]\tNon-cancer illness code, self-reported | Instance 1 | Array 0\n",
      "✓ [p20002_i1_a1]\tNon-cancer illness code, self-reported | Instance 1 | Array 1\n",
      "✓ [p20002_i1_a2]\tNon-cancer illness code, self-reported | Instance 1 | Array 2\n",
      "✓ [p20002_i1_a3]\tNon-cancer illness code, self-reported | Instance 1 | Array 3\n",
      "✓ [p20002_i1_a4]\tNon-cancer illness code, self-reported | Instance 1 | Array 4\n",
      "✓ [p20002_i1_a5]\tNon-cancer illness code, self-reported | Instance 1 | Array 5\n",
      "✓ [p20002_i1_a6]\tNon-cancer illness code, self-reported | Instance 1 | Array 6\n",
      "✓ [p20002_i1_a7]\tNon-cancer illness code, self-reported | Instance 1 | Array 7\n",
      "✓ [p20002_i1_a8]\tNon-cancer illness code, self-reported | Instance 1 | Array 8\n",
      "✓ [p20002_i1_a9]\tNon-cancer illness code, self-reported | Instance 1 | Array 9\n",
      "✓ [p20002_i1_a10]\tNon-cancer illness code, self-reported | Instance 1 | Array 10\n",
      "✓ [p20002_i1_a11]\tNon-cancer illness code, self-reported | Instance 1 | Array 11\n",
      "✓ [p20002_i1_a12]\tNon-cancer illness code, self-reported | Instance 1 | Array 12\n",
      "✓ [p20002_i1_a13]\tNon-cancer illness code, self-reported | Instance 1 | Array 13\n",
      "✓ [p20002_i1_a14]\tNon-cancer illness code, self-reported | Instance 1 | Array 14\n",
      "✓ [p20002_i1_a15]\tNon-cancer illness code, self-reported | Instance 1 | Array 15\n",
      "✓ [p20002_i1_a16]\tNon-cancer illness code, self-reported | Instance 1 | Array 16\n",
      "✓ [p20002_i1_a17]\tNon-cancer illness code, self-reported | Instance 1 | Array 17\n",
      "✓ [p20002_i1_a18]\tNon-cancer illness code, self-reported | Instance 1 | Array 18\n",
      "✓ [p20002_i1_a19]\tNon-cancer illness code, self-reported | Instance 1 | Array 19\n",
      "✓ [p20002_i1_a20]\tNon-cancer illness code, self-reported | Instance 1 | Array 20\n",
      "✓ [p20002_i1_a21]\tNon-cancer illness code, self-reported | Instance 1 | Array 21\n",
      "✓ [p20002_i1_a22]\tNon-cancer illness code, self-reported | Instance 1 | Array 22\n",
      "✓ [p20002_i1_a23]\tNon-cancer illness code, self-reported | Instance 1 | Array 23\n",
      "✓ [p20002_i1_a24]\tNon-cancer illness code, self-reported | Instance 1 | Array 24\n",
      "✓ [p20002_i1_a25]\tNon-cancer illness code, self-reported | Instance 1 | Array 25\n",
      "✓ [p20002_i1_a26]\tNon-cancer illness code, self-reported | Instance 1 | Array 26\n",
      "✓ [p20002_i1_a27]\tNon-cancer illness code, self-reported | Instance 1 | Array 27\n",
      "✓ [p20002_i1_a28]\tNon-cancer illness code, self-reported | Instance 1 | Array 28\n",
      "✓ [p20002_i1_a29]\tNon-cancer illness code, self-reported | Instance 1 | Array 29\n",
      "✓ [p20002_i1_a30]\tNon-cancer illness code, self-reported | Instance 1 | Array 30\n",
      "✓ [p20002_i1_a31]\tNon-cancer illness code, self-reported | Instance 1 | Array 31\n",
      "✓ [p20002_i1_a32]\tNon-cancer illness code, self-reported | Instance 1 | Array 32\n",
      "✓ [p20002_i1_a33]\tNon-cancer illness code, self-reported | Instance 1 | Array 33\n",
      "✓ [p20002_i2_a0]\tNon-cancer illness code, self-reported | Instance 2 | Array 0\n",
      "✓ [p20002_i2_a1]\tNon-cancer illness code, self-reported | Instance 2 | Array 1\n",
      "✓ [p20002_i2_a2]\tNon-cancer illness code, self-reported | Instance 2 | Array 2\n",
      "✓ [p20002_i2_a3]\tNon-cancer illness code, self-reported | Instance 2 | Array 3\n",
      "✓ [p20002_i2_a4]\tNon-cancer illness code, self-reported | Instance 2 | Array 4\n",
      "✓ [p20002_i2_a5]\tNon-cancer illness code, self-reported | Instance 2 | Array 5\n",
      "✓ [p20002_i2_a6]\tNon-cancer illness code, self-reported | Instance 2 | Array 6\n",
      "✓ [p20002_i2_a7]\tNon-cancer illness code, self-reported | Instance 2 | Array 7\n",
      "✓ [p20002_i2_a8]\tNon-cancer illness code, self-reported | Instance 2 | Array 8\n",
      "✓ [p20002_i2_a9]\tNon-cancer illness code, self-reported | Instance 2 | Array 9\n",
      "✓ [p20002_i2_a10]\tNon-cancer illness code, self-reported | Instance 2 | Array 10\n",
      "✓ [p20002_i2_a11]\tNon-cancer illness code, self-reported | Instance 2 | Array 11\n",
      "✓ [p20002_i2_a12]\tNon-cancer illness code, self-reported | Instance 2 | Array 12\n",
      "✓ [p20002_i2_a13]\tNon-cancer illness code, self-reported | Instance 2 | Array 13\n",
      "✓ [p20002_i2_a14]\tNon-cancer illness code, self-reported | Instance 2 | Array 14\n",
      "✓ [p20002_i2_a15]\tNon-cancer illness code, self-reported | Instance 2 | Array 15\n",
      "✓ [p20002_i2_a16]\tNon-cancer illness code, self-reported | Instance 2 | Array 16\n",
      "✓ [p20002_i2_a17]\tNon-cancer illness code, self-reported | Instance 2 | Array 17\n",
      "✓ [p20002_i2_a18]\tNon-cancer illness code, self-reported | Instance 2 | Array 18\n",
      "✓ [p20002_i2_a19]\tNon-cancer illness code, self-reported | Instance 2 | Array 19\n",
      "✓ [p20002_i2_a20]\tNon-cancer illness code, self-reported | Instance 2 | Array 20\n",
      "✓ [p20002_i2_a21]\tNon-cancer illness code, self-reported | Instance 2 | Array 21\n",
      "✓ [p20002_i2_a22]\tNon-cancer illness code, self-reported | Instance 2 | Array 22\n",
      "✓ [p20002_i2_a23]\tNon-cancer illness code, self-reported | Instance 2 | Array 23\n",
      "✓ [p20002_i2_a24]\tNon-cancer illness code, self-reported | Instance 2 | Array 24\n",
      "✓ [p20002_i2_a25]\tNon-cancer illness code, self-reported | Instance 2 | Array 25\n",
      "✓ [p20002_i2_a26]\tNon-cancer illness code, self-reported | Instance 2 | Array 26\n",
      "✓ [p20002_i2_a27]\tNon-cancer illness code, self-reported | Instance 2 | Array 27\n",
      "✓ [p20002_i2_a28]\tNon-cancer illness code, self-reported | Instance 2 | Array 28\n",
      "✓ [p20002_i2_a29]\tNon-cancer illness code, self-reported | Instance 2 | Array 29\n",
      "✓ [p20002_i2_a30]\tNon-cancer illness code, self-reported | Instance 2 | Array 30\n",
      "✓ [p20002_i2_a31]\tNon-cancer illness code, self-reported | Instance 2 | Array 31\n",
      "✓ [p20002_i2_a32]\tNon-cancer illness code, self-reported | Instance 2 | Array 32\n",
      "✓ [p20002_i2_a33]\tNon-cancer illness code, self-reported | Instance 2 | Array 33\n",
      "✓ [p20002_i3_a0]\tNon-cancer illness code, self-reported | Instance 3 | Array 0\n",
      "✓ [p20002_i3_a1]\tNon-cancer illness code, self-reported | Instance 3 | Array 1\n",
      "✓ [p20002_i3_a2]\tNon-cancer illness code, self-reported | Instance 3 | Array 2\n",
      "✓ [p20002_i3_a3]\tNon-cancer illness code, self-reported | Instance 3 | Array 3\n",
      "✓ [p20002_i3_a4]\tNon-cancer illness code, self-reported | Instance 3 | Array 4\n",
      "✓ [p20002_i3_a5]\tNon-cancer illness code, self-reported | Instance 3 | Array 5\n",
      "✓ [p20002_i3_a6]\tNon-cancer illness code, self-reported | Instance 3 | Array 6\n",
      "✓ [p20002_i3_a7]\tNon-cancer illness code, self-reported | Instance 3 | Array 7\n",
      "✓ [p20002_i3_a8]\tNon-cancer illness code, self-reported | Instance 3 | Array 8\n",
      "✓ [p20002_i3_a9]\tNon-cancer illness code, self-reported | Instance 3 | Array 9\n",
      "✓ [p20002_i3_a10]\tNon-cancer illness code, self-reported | Instance 3 | Array 10\n",
      "✓ [p20002_i3_a11]\tNon-cancer illness code, self-reported | Instance 3 | Array 11\n",
      "✓ [p20002_i3_a12]\tNon-cancer illness code, self-reported | Instance 3 | Array 12\n",
      "✓ [p20002_i3_a13]\tNon-cancer illness code, self-reported | Instance 3 | Array 13\n",
      "✓ [p20002_i3_a14]\tNon-cancer illness code, self-reported | Instance 3 | Array 14\n",
      "✓ [p20002_i3_a15]\tNon-cancer illness code, self-reported | Instance 3 | Array 15\n",
      "✓ [p20002_i3_a16]\tNon-cancer illness code, self-reported | Instance 3 | Array 16\n",
      "✓ [p20002_i3_a17]\tNon-cancer illness code, self-reported | Instance 3 | Array 17\n",
      "✓ [p20002_i3_a18]\tNon-cancer illness code, self-reported | Instance 3 | Array 18\n",
      "✓ [p20002_i3_a19]\tNon-cancer illness code, self-reported | Instance 3 | Array 19\n",
      "✓ [p20002_i3_a20]\tNon-cancer illness code, self-reported | Instance 3 | Array 20\n",
      "✓ [p20002_i3_a21]\tNon-cancer illness code, self-reported | Instance 3 | Array 21\n",
      "✓ [p20002_i3_a22]\tNon-cancer illness code, self-reported | Instance 3 | Array 22\n",
      "✓ [p20002_i3_a23]\tNon-cancer illness code, self-reported | Instance 3 | Array 23\n",
      "✓ [p20002_i3_a24]\tNon-cancer illness code, self-reported | Instance 3 | Array 24\n",
      "✓ [p20002_i3_a25]\tNon-cancer illness code, self-reported | Instance 3 | Array 25\n",
      "✓ [p20002_i3_a26]\tNon-cancer illness code, self-reported | Instance 3 | Array 26\n",
      "✓ [p20002_i3_a27]\tNon-cancer illness code, self-reported | Instance 3 | Array 27\n",
      "✓ [p20002_i3_a28]\tNon-cancer illness code, self-reported | Instance 3 | Array 28\n",
      "✓ [p20002_i3_a29]\tNon-cancer illness code, self-reported | Instance 3 | Array 29\n",
      "✓ [p20002_i3_a30]\tNon-cancer illness code, self-reported | Instance 3 | Array 30\n",
      "✓ [p20002_i3_a31]\tNon-cancer illness code, self-reported | Instance 3 | Array 31\n",
      "✓ [p20002_i3_a32]\tNon-cancer illness code, self-reported | Instance 3 | Array 32\n",
      "✓ [p20002_i3_a33]\tNon-cancer illness code, self-reported | Instance 3 | Array 33\n",
      "  → Found 136 instances for p20002\n",
      "✓ [p20116_i0]\tSmoking status | Instance 0\n",
      "✓ [p20116_i1]\tSmoking status | Instance 1\n",
      "✓ [p20116_i2]\tSmoking status | Instance 2\n",
      "✓ [p20116_i3]\tSmoking status | Instance 3\n",
      "  → Found 4 instances for p20116\n",
      "✓ [p20126_i0]\tBipolar and major depression status | Instance 0\n",
      "  → Found 1 instances for p20126\n",
      "✓ [p20205_i2]\tECG datasets | Instance 2\n",
      "✓ [p20205_i3]\tECG datasets | Instance 3\n",
      "  → Found 2 instances for p20205\n",
      "✓ [p20208_i2]\tLong axis heart images - DICOM | Instance 2\n",
      "✓ [p20208_i3]\tLong axis heart images - DICOM | Instance 3\n",
      "  → Found 2 instances for p20208\n",
      "✓ [p20252_i2]\tT1 structural brain images - NIFTI | Instance 2\n",
      "✓ [p20252_i3]\tT1 structural brain images - NIFTI | Instance 3\n",
      "  → Found 2 instances for p20252\n",
      "✓ [p20255]\tSpirometry QC measure\n",
      "✓ [p20256]\tForced expiratory volume in 1-second (FEV1) Z-score\n",
      "✓ [p20257]\tForced vital capacity (FVC) Z-score\n",
      "✓ [p20258]\tFEV1/ FVC ratio Z-score\n",
      "✓ [p2090_i0]\tSeen doctor (GP) for nerves, anxiety, tension or depression | Instance 0\n",
      "✓ [p2090_i1]\tSeen doctor (GP) for nerves, anxiety, tension or depression | Instance 1\n",
      "✓ [p2090_i2]\tSeen doctor (GP) for nerves, anxiety, tension or depression | Instance 2\n",
      "✓ [p2090_i3]\tSeen doctor (GP) for nerves, anxiety, tension or depression | Instance 3\n",
      "  → Found 4 instances for p2090\n",
      "✓ [p2100_i0]\tSeen a psychiatrist for nerves, anxiety, tension or depression | Instance 0\n",
      "✓ [p2100_i1]\tSeen a psychiatrist for nerves, anxiety, tension or depression | Instance 1\n",
      "✓ [p2100_i2]\tSeen a psychiatrist for nerves, anxiety, tension or depression | Instance 2\n",
      "✓ [p2100_i3]\tSeen a psychiatrist for nerves, anxiety, tension or depression | Instance 3\n",
      "  → Found 4 instances for p2100\n",
      "✓ [p21000_i0]\tEthnic background | Instance 0\n",
      "✓ [p21000_i1]\tEthnic background | Instance 1\n",
      "✓ [p21000_i2]\tEthnic background | Instance 2\n",
      "✓ [p21000_i3]\tEthnic background | Instance 3\n",
      "  → Found 4 instances for p21000\n",
      "✓ [p21001_i0]\tBody mass index (BMI) | Instance 0\n",
      "✓ [p21001_i1]\tBody mass index (BMI) | Instance 1\n",
      "✓ [p21001_i2]\tBody mass index (BMI) | Instance 2\n",
      "✓ [p21001_i3]\tBody mass index (BMI) | Instance 3\n",
      "  → Found 4 instances for p21001\n",
      "✓ [p22506]\tTobacco smoking\n",
      "✓ [p23104_i0]\tBody mass index (BMI) | Instance 0\n",
      "✓ [p23104_i1]\tBody mass index (BMI) | Instance 1\n",
      "✓ [p23104_i2]\tBody mass index (BMI) | Instance 2\n",
      "✓ [p23104_i3]\tBody mass index (BMI) | Instance 3\n",
      "  → Found 4 instances for p23104\n",
      "✓ [p2316_i0]\tWheeze or whistling in the chest in last year | Instance 0\n",
      "✓ [p2316_i1]\tWheeze or whistling in the chest in last year | Instance 1\n",
      "✓ [p2316_i2]\tWheeze or whistling in the chest in last year | Instance 2\n",
      "✓ [p2316_i3]\tWheeze or whistling in the chest in last year | Instance 3\n",
      "  → Found 4 instances for p2316\n",
      "✓ [p2335_i0]\tChest pain or discomfort | Instance 0\n",
      "✓ [p2335_i1]\tChest pain or discomfort | Instance 1\n",
      "✓ [p2335_i2]\tChest pain or discomfort | Instance 2\n",
      "✓ [p2335_i3]\tChest pain or discomfort | Instance 3\n",
      "  → Found 4 instances for p2335\n",
      "✓ [p24003]\tNitrogen dioxide air pollution; 2010\n",
      "✓ [p24004]\tNitrogen oxides air pollution; 2010\n",
      "✓ [p24005]\tParticulate matter air pollution (pm10); 2010\n",
      "✓ [p24006]\tParticulate matter air pollution (pm2.5); 2010\n",
      "✓ [p24016]\tNitrogen dioxide air pollution; 2005\n",
      "✓ [p2867_i0]\tAge started smoking in former smokers | Instance 0\n",
      "✓ [p2867_i1]\tAge started smoking in former smokers | Instance 1\n",
      "✓ [p2867_i2]\tAge started smoking in former smokers | Instance 2\n",
      "✓ [p2867_i3]\tAge started smoking in former smokers | Instance 3\n",
      "  → Found 4 instances for p2867\n",
      "✓ [p2887_i0]\tNumber of cigarettes previously smoked daily | Instance 0\n",
      "✓ [p2887_i1]\tNumber of cigarettes previously smoked daily | Instance 1\n",
      "✓ [p2887_i2]\tNumber of cigarettes previously smoked daily | Instance 2\n",
      "✓ [p2887_i3]\tNumber of cigarettes previously smoked daily | Instance 3\n",
      "  → Found 4 instances for p2887\n",
      "✓ [p30000_i0]\tWhite blood cell (leukocyte) count | Instance 0\n",
      "✓ [p30000_i1]\tWhite blood cell (leukocyte) count | Instance 1\n",
      "✓ [p30000_i2]\tWhite blood cell (leukocyte) count | Instance 2\n",
      "  → Found 3 instances for p30000\n",
      "✓ [p30010_i0]\tRed blood cell (erythrocyte) count | Instance 0\n",
      "✓ [p30010_i1]\tRed blood cell (erythrocyte) count | Instance 1\n",
      "✓ [p30010_i2]\tRed blood cell (erythrocyte) count | Instance 2\n",
      "  → Found 3 instances for p30010\n",
      "✓ [p30020_i0]\tHaemoglobin concentration | Instance 0\n",
      "✓ [p30020_i1]\tHaemoglobin concentration | Instance 1\n",
      "✓ [p30020_i2]\tHaemoglobin concentration | Instance 2\n",
      "  → Found 3 instances for p30020\n",
      "✓ [p30030_i0]\tHaematocrit percentage | Instance 0\n",
      "✓ [p30030_i1]\tHaematocrit percentage | Instance 1\n",
      "✓ [p30030_i2]\tHaematocrit percentage | Instance 2\n",
      "  → Found 3 instances for p30030\n",
      "✓ [p30040_i0]\tMean corpuscular volume | Instance 0\n",
      "✓ [p30040_i1]\tMean corpuscular volume | Instance 1\n",
      "✓ [p30040_i2]\tMean corpuscular volume | Instance 2\n",
      "  → Found 3 instances for p30040\n",
      "✓ [p30050_i0]\tMean corpuscular haemoglobin | Instance 0\n",
      "✓ [p30050_i1]\tMean corpuscular haemoglobin | Instance 1\n",
      "✓ [p30050_i2]\tMean corpuscular haemoglobin | Instance 2\n",
      "  → Found 3 instances for p30050\n",
      "✓ [p30060_i0]\tMean corpuscular haemoglobin concentration | Instance 0\n",
      "✓ [p30060_i1]\tMean corpuscular haemoglobin concentration | Instance 1\n",
      "✓ [p30060_i2]\tMean corpuscular haemoglobin concentration | Instance 2\n",
      "  → Found 3 instances for p30060\n",
      "✓ [p30070_i0]\tRed blood cell (erythrocyte) distribution width | Instance 0\n",
      "✓ [p30070_i1]\tRed blood cell (erythrocyte) distribution width | Instance 1\n",
      "✓ [p30070_i2]\tRed blood cell (erythrocyte) distribution width | Instance 2\n",
      "  → Found 3 instances for p30070\n",
      "✓ [p30080_i0]\tPlatelet count | Instance 0\n",
      "✓ [p30080_i1]\tPlatelet count | Instance 1\n",
      "✓ [p30080_i2]\tPlatelet count | Instance 2\n",
      "  → Found 3 instances for p30080\n",
      "✓ [p30090_i0]\tPlatelet crit | Instance 0\n",
      "✓ [p30090_i1]\tPlatelet crit | Instance 1\n",
      "✓ [p30090_i2]\tPlatelet crit | Instance 2\n",
      "  → Found 3 instances for p30090\n",
      "✓ [p30100_i0]\tMean platelet (thrombocyte) volume | Instance 0\n",
      "✓ [p30100_i1]\tMean platelet (thrombocyte) volume | Instance 1\n",
      "✓ [p30100_i2]\tMean platelet (thrombocyte) volume | Instance 2\n",
      "  → Found 3 instances for p30100\n",
      "✓ [p30110_i0]\tPlatelet distribution width | Instance 0\n",
      "✓ [p30110_i1]\tPlatelet distribution width | Instance 1\n",
      "✓ [p30110_i2]\tPlatelet distribution width | Instance 2\n",
      "  → Found 3 instances for p30110\n",
      "✓ [p30120_i0]\tLymphocyte count | Instance 0\n",
      "✓ [p30120_i1]\tLymphocyte count | Instance 1\n",
      "✓ [p30120_i2]\tLymphocyte count | Instance 2\n",
      "  → Found 3 instances for p30120\n",
      "✓ [p30130_i0]\tMonocyte count | Instance 0\n",
      "✓ [p30130_i1]\tMonocyte count | Instance 1\n",
      "✓ [p30130_i2]\tMonocyte count | Instance 2\n",
      "  → Found 3 instances for p30130\n",
      "✓ [p30140_i0]\tNeutrophill count | Instance 0\n",
      "✓ [p30140_i1]\tNeutrophill count | Instance 1\n",
      "✓ [p30140_i2]\tNeutrophill count | Instance 2\n",
      "  → Found 3 instances for p30140\n",
      "✓ [p30150_i0]\tEosinophill count | Instance 0\n",
      "✓ [p30150_i1]\tEosinophill count | Instance 1\n",
      "✓ [p30150_i2]\tEosinophill count | Instance 2\n",
      "  → Found 3 instances for p30150\n",
      "✓ [p30600_i0]\tAlbumin | Instance 0\n",
      "✓ [p30600_i1]\tAlbumin | Instance 1\n",
      "  → Found 2 instances for p30600\n",
      "✓ [p30610_i0]\tAlkaline phosphatase | Instance 0\n",
      "✓ [p30610_i1]\tAlkaline phosphatase | Instance 1\n",
      "  → Found 2 instances for p30610\n",
      "✓ [p30620_i0]\tAlanine aminotransferase | Instance 0\n",
      "✓ [p30620_i1]\tAlanine aminotransferase | Instance 1\n",
      "  → Found 2 instances for p30620\n",
      "✓ [p3063_i0_a0]\tForced expiratory volume in 1-second (FEV1) | Instance 0 | Array 0\n",
      "✓ [p3063_i0_a1]\tForced expiratory volume in 1-second (FEV1) | Instance 0 | Array 1\n",
      "✓ [p3063_i0_a2]\tForced expiratory volume in 1-second (FEV1) | Instance 0 | Array 2\n",
      "✓ [p3063_i1_a0]\tForced expiratory volume in 1-second (FEV1) | Instance 1 | Array 0\n",
      "✓ [p3063_i1_a1]\tForced expiratory volume in 1-second (FEV1) | Instance 1 | Array 1\n",
      "✓ [p3063_i1_a2]\tForced expiratory volume in 1-second (FEV1) | Instance 1 | Array 2\n",
      "✓ [p3063_i2_a0]\tForced expiratory volume in 1-second (FEV1) | Instance 2 | Array 0\n",
      "✓ [p3063_i2_a1]\tForced expiratory volume in 1-second (FEV1) | Instance 2 | Array 1\n",
      "✓ [p3063_i2_a2]\tForced expiratory volume in 1-second (FEV1) | Instance 2 | Array 2\n",
      "✓ [p3063_i3_a0]\tForced expiratory volume in 1-second (FEV1) | Instance 3 | Array 0\n",
      "✓ [p3063_i3_a1]\tForced expiratory volume in 1-second (FEV1) | Instance 3 | Array 1\n",
      "✓ [p3063_i3_a2]\tForced expiratory volume in 1-second (FEV1) | Instance 3 | Array 2\n",
      "  → Found 12 instances for p3063\n",
      "✓ [p30630_i0]\tApolipoprotein A | Instance 0\n",
      "✓ [p30630_i1]\tApolipoprotein A | Instance 1\n",
      "  → Found 2 instances for p30630\n",
      "✓ [p3064_i0_a0]\tPeak expiratory flow (PEF) | Instance 0 | Array 0\n",
      "✓ [p3064_i0_a1]\tPeak expiratory flow (PEF) | Instance 0 | Array 1\n",
      "✓ [p3064_i0_a2]\tPeak expiratory flow (PEF) | Instance 0 | Array 2\n",
      "✓ [p3064_i1_a0]\tPeak expiratory flow (PEF) | Instance 1 | Array 0\n",
      "✓ [p3064_i1_a1]\tPeak expiratory flow (PEF) | Instance 1 | Array 1\n",
      "✓ [p3064_i1_a2]\tPeak expiratory flow (PEF) | Instance 1 | Array 2\n",
      "✓ [p3064_i2_a0]\tPeak expiratory flow (PEF) | Instance 2 | Array 0\n",
      "✓ [p3064_i2_a1]\tPeak expiratory flow (PEF) | Instance 2 | Array 1\n",
      "✓ [p3064_i2_a2]\tPeak expiratory flow (PEF) | Instance 2 | Array 2\n",
      "✓ [p3064_i3_a0]\tPeak expiratory flow (PEF) | Instance 3 | Array 0\n",
      "✓ [p3064_i3_a1]\tPeak expiratory flow (PEF) | Instance 3 | Array 1\n",
      "✓ [p3064_i3_a2]\tPeak expiratory flow (PEF) | Instance 3 | Array 2\n",
      "  → Found 12 instances for p3064\n",
      "✓ [p30640_i0]\tApolipoprotein B | Instance 0\n",
      "✓ [p30640_i1]\tApolipoprotein B | Instance 1\n",
      "  → Found 2 instances for p30640\n",
      "✓ [p30650_i0]\tAspartate aminotransferase | Instance 0\n",
      "✓ [p30650_i1]\tAspartate aminotransferase | Instance 1\n",
      "  → Found 2 instances for p30650\n",
      "✓ [p30660_i0]\tDirect bilirubin | Instance 0\n",
      "✓ [p30660_i1]\tDirect bilirubin | Instance 1\n",
      "  → Found 2 instances for p30660\n",
      "✓ [p30670_i0]\tUrea | Instance 0\n",
      "✓ [p30670_i1]\tUrea | Instance 1\n",
      "  → Found 2 instances for p30670\n",
      "✓ [p30680_i0]\tCalcium | Instance 0\n",
      "✓ [p30680_i1]\tCalcium | Instance 1\n",
      "  → Found 2 instances for p30680\n",
      "✓ [p30690_i0]\tCholesterol | Instance 0\n",
      "✓ [p30690_i1]\tCholesterol | Instance 1\n",
      "  → Found 2 instances for p30690\n",
      "✓ [p30700_i0]\tCreatinine | Instance 0\n",
      "✓ [p30700_i1]\tCreatinine | Instance 1\n",
      "  → Found 2 instances for p30700\n",
      "✓ [p30710_i0]\tC-reactive protein | Instance 0\n",
      "✓ [p30710_i1]\tC-reactive protein | Instance 1\n",
      "  → Found 2 instances for p30710\n",
      "✓ [p30720_i0]\tCystatin C | Instance 0\n",
      "✓ [p30720_i1]\tCystatin C | Instance 1\n",
      "  → Found 2 instances for p30720\n",
      "✓ [p30730_i0]\tGamma glutamyltransferase | Instance 0\n",
      "✓ [p30730_i1]\tGamma glutamyltransferase | Instance 1\n",
      "  → Found 2 instances for p30730\n",
      "✓ [p30740_i0]\tGlucose | Instance 0\n",
      "✓ [p30740_i1]\tGlucose | Instance 1\n",
      "  → Found 2 instances for p30740\n",
      "✓ [p30750_i0]\tGlycated haemoglobin (HbA1c) | Instance 0\n",
      "✓ [p30750_i1]\tGlycated haemoglobin (HbA1c) | Instance 1\n",
      "  → Found 2 instances for p30750\n",
      "✓ [p30760_i0]\tHDL cholesterol | Instance 0\n",
      "✓ [p30760_i1]\tHDL cholesterol | Instance 1\n",
      "  → Found 2 instances for p30760\n",
      "✓ [p30770_i0]\tIGF-1 | Instance 0\n",
      "✓ [p30770_i1]\tIGF-1 | Instance 1\n",
      "  → Found 2 instances for p30770\n",
      "✓ [p30780_i0]\tLDL direct | Instance 0\n",
      "✓ [p30780_i1]\tLDL direct | Instance 1\n",
      "  → Found 2 instances for p30780\n",
      "✓ [p30790_i0]\tLipoprotein A | Instance 0\n",
      "✓ [p30790_i1]\tLipoprotein A | Instance 1\n",
      "  → Found 2 instances for p30790\n",
      "✓ [p30800_i0]\tOestradiol | Instance 0\n",
      "✓ [p30800_i1]\tOestradiol | Instance 1\n",
      "  → Found 2 instances for p30800\n",
      "✓ [p30810_i0]\tPhosphate | Instance 0\n",
      "✓ [p30810_i1]\tPhosphate | Instance 1\n",
      "  → Found 2 instances for p30810\n",
      "✓ [p31]\tSex\n",
      "✓ [p34]\tYear of birth\n",
      "✓ [p3786_i0]\tAge asthma diagnosed | Instance 0\n",
      "✓ [p3786_i1]\tAge asthma diagnosed | Instance 1\n",
      "✓ [p3786_i2]\tAge asthma diagnosed | Instance 2\n",
      "✓ [p3786_i3]\tAge asthma diagnosed | Instance 3\n",
      "  → Found 4 instances for p3786\n",
      "✓ [p40001_i0]\tUnderlying (primary) cause of death: ICD10 | Instance 0\n",
      "✓ [p40001_i1]\tUnderlying (primary) cause of death: ICD10 | Instance 1\n",
      "  → Found 2 instances for p40001\n",
      "✓ [p40002_i0_a0]\tContributory (secondary) causes of death: ICD10 | Instance 0 | Array 0\n",
      "✓ [p40002_i0_a1]\tContributory (secondary) causes of death: ICD10 | Instance 0 | Array 1\n",
      "✓ [p40002_i0_a2]\tContributory (secondary) causes of death: ICD10 | Instance 0 | Array 2\n",
      "✓ [p40002_i0_a3]\tContributory (secondary) causes of death: ICD10 | Instance 0 | Array 3\n",
      "✓ [p40002_i0_a4]\tContributory (secondary) causes of death: ICD10 | Instance 0 | Array 4\n",
      "✓ [p40002_i0_a5]\tContributory (secondary) causes of death: ICD10 | Instance 0 | Array 5\n",
      "✓ [p40002_i0_a6]\tContributory (secondary) causes of death: ICD10 | Instance 0 | Array 6\n",
      "✓ [p40002_i0_a7]\tContributory (secondary) causes of death: ICD10 | Instance 0 | Array 7\n",
      "✓ [p40002_i0_a8]\tContributory (secondary) causes of death: ICD10 | Instance 0 | Array 8\n",
      "✓ [p40002_i0_a9]\tContributory (secondary) causes of death: ICD10 | Instance 0 | Array 9\n",
      "✓ [p40002_i0_a10]\tContributory (secondary) causes of death: ICD10 | Instance 0 | Array 10\n",
      "✓ [p40002_i0_a11]\tContributory (secondary) causes of death: ICD10 | Instance 0 | Array 11\n",
      "✓ [p40002_i0_a12]\tContributory (secondary) causes of death: ICD10 | Instance 0 | Array 12\n",
      "✓ [p40002_i0_a13]\tContributory (secondary) causes of death: ICD10 | Instance 0 | Array 13\n",
      "✓ [p40002_i0_a14]\tContributory (secondary) causes of death: ICD10 | Instance 0 | Array 14\n",
      "✓ [p40002_i1_a0]\tContributory (secondary) causes of death: ICD10 | Instance 1 | Array 0\n",
      "✓ [p40002_i1_a1]\tContributory (secondary) causes of death: ICD10 | Instance 1 | Array 1\n",
      "✓ [p40002_i1_a2]\tContributory (secondary) causes of death: ICD10 | Instance 1 | Array 2\n",
      "✓ [p40002_i1_a3]\tContributory (secondary) causes of death: ICD10 | Instance 1 | Array 3\n",
      "✓ [p40002_i1_a4]\tContributory (secondary) causes of death: ICD10 | Instance 1 | Array 4\n",
      "✓ [p40002_i1_a5]\tContributory (secondary) causes of death: ICD10 | Instance 1 | Array 5\n",
      "✓ [p40002_i1_a6]\tContributory (secondary) causes of death: ICD10 | Instance 1 | Array 6\n",
      "✓ [p40002_i1_a7]\tContributory (secondary) causes of death: ICD10 | Instance 1 | Array 7\n",
      "✓ [p40002_i1_a8]\tContributory (secondary) causes of death: ICD10 | Instance 1 | Array 8\n",
      "✓ [p40002_i1_a9]\tContributory (secondary) causes of death: ICD10 | Instance 1 | Array 9\n",
      "✓ [p40002_i1_a10]\tContributory (secondary) causes of death: ICD10 | Instance 1 | Array 10\n",
      "✓ [p40002_i1_a11]\tContributory (secondary) causes of death: ICD10 | Instance 1 | Array 11\n",
      "✓ [p40002_i1_a12]\tContributory (secondary) causes of death: ICD10 | Instance 1 | Array 12\n",
      "✓ [p40002_i1_a13]\tContributory (secondary) causes of death: ICD10 | Instance 1 | Array 13\n",
      "✓ [p40002_i1_a14]\tContributory (secondary) causes of death: ICD10 | Instance 1 | Array 14\n",
      "  → Found 30 instances for p40002\n",
      "✓ [p41202]\tDiagnoses - main ICD10\n",
      "✓ [p41204]\tDiagnoses - secondary ICD10\n",
      "✓ [p41270]\tDiagnoses - ICD10\n",
      "✓ [p4559_i0]\tFamily relationship satisfaction | Instance 0\n",
      "✓ [p4559_i1]\tFamily relationship satisfaction | Instance 1\n",
      "✓ [p4559_i2]\tFamily relationship satisfaction | Instance 2\n",
      "✓ [p4559_i3]\tFamily relationship satisfaction | Instance 3\n",
      "  → Found 4 instances for p4559\n",
      "✓ [p4598_i0]\tEver depressed for a whole week | Instance 0\n",
      "✓ [p4598_i1]\tEver depressed for a whole week | Instance 1\n",
      "✓ [p4598_i2]\tEver depressed for a whole week | Instance 2\n",
      "✓ [p4598_i3]\tEver depressed for a whole week | Instance 3\n",
      "  → Found 4 instances for p4598\n",
      "✓ [p4631_i0]\tEver unenthusiastic/disinterested for a whole week | Instance 0\n",
      "✓ [p4631_i1]\tEver unenthusiastic/disinterested for a whole week | Instance 1\n",
      "✓ [p4631_i2]\tEver unenthusiastic/disinterested for a whole week | Instance 2\n",
      "✓ [p4631_i3]\tEver unenthusiastic/disinterested for a whole week | Instance 3\n",
      "  → Found 4 instances for p4631\n",
      "✓ [p4717_i0]\tShortness of breath walking on level ground | Instance 0\n",
      "✓ [p4717_i1]\tShortness of breath walking on level ground | Instance 1\n",
      "✓ [p4717_i2]\tShortness of breath walking on level ground | Instance 2\n",
      "✓ [p4717_i3]\tShortness of breath walking on level ground | Instance 3\n",
      "  → Found 4 instances for p4717\n",
      "✓ [p50_i0]\tStanding height | Instance 0\n",
      "✓ [p50_i1]\tStanding height | Instance 1\n",
      "✓ [p50_i2]\tStanding height | Instance 2\n",
      "✓ [p50_i3]\tStanding height | Instance 3\n",
      "  → Found 4 instances for p50\n",
      "✓ [p52]\tMonth of birth\n",
      "✓ [p5375_i0]\tLongest period of unenthusiasm / disinterest | Instance 0\n",
      "✓ [p5375_i1]\tLongest period of unenthusiasm / disinterest | Instance 1\n",
      "✓ [p5375_i2]\tLongest period of unenthusiasm / disinterest | Instance 2\n",
      "✓ [p5375_i3]\tLongest period of unenthusiasm / disinterest | Instance 3\n",
      "  → Found 4 instances for p5375\n",
      "✓ [p5663_i0]\tLength of longest manic/irritable episode | Instance 0\n",
      "✓ [p5663_i1]\tLength of longest manic/irritable episode | Instance 1\n",
      "✓ [p5663_i2]\tLength of longest manic/irritable episode | Instance 2\n",
      "✓ [p5663_i3]\tLength of longest manic/irritable episode | Instance 3\n",
      "  → Found 4 instances for p5663\n",
      "✓ [p6138_i0]\tQualifications | Instance 0\n",
      "✓ [p6138_i1]\tQualifications | Instance 1\n",
      "✓ [p6138_i2]\tQualifications | Instance 2\n",
      "✓ [p6138_i3]\tQualifications | Instance 3\n",
      "  → Found 4 instances for p6138\n",
      "✓ [p6152_i0]\tBlood clot, DVT, bronchitis, emphysema, asthma, rhinitis, eczema, allergy diagnosed by doctor | Instance 0\n",
      "✓ [p6152_i1]\tBlood clot, DVT, bronchitis, emphysema, asthma, rhinitis, eczema, allergy diagnosed by doctor | Instance 1\n",
      "✓ [p6152_i2]\tBlood clot, DVT, bronchitis, emphysema, asthma, rhinitis, eczema, allergy diagnosed by doctor | Instance 2\n",
      "✓ [p6152_i3]\tBlood clot, DVT, bronchitis, emphysema, asthma, rhinitis, eczema, allergy diagnosed by doctor | Instance 3\n",
      "  → Found 4 instances for p6152\n",
      "✓ [p738_i0]\tAverage total household income before tax | Instance 0\n",
      "✓ [p738_i1]\tAverage total household income before tax | Instance 1\n",
      "✓ [p738_i2]\tAverage total household income before tax | Instance 2\n",
      "✓ [p738_i3]\tAverage total household income before tax | Instance 3\n",
      "  → Found 4 instances for p738\n",
      "\n",
      "\n",
      "Summary:\n",
      "Found: 398 fields (including all instances)\n",
      "Missing: 0 base fields\n"
     ]
    }
   ],
   "source": [
    "# Verify fields and get their information (including ALL instances)\n",
    "# Use fuzzy matching to find all instances of each field (e.g., p50, p50_i0, p50_i1, etc.)\n",
    "\n",
    "verified_fields = []\n",
    "missing_fields = []\n",
    "\n",
    "for field_name in field_names:\n",
    "    try:\n",
    "        # Try exact match first (for fields without instances)\n",
    "        try:\n",
    "            field = pheno.find_field(name=field_name)\n",
    "            verified_fields.append({\n",
    "                'name': field.name,\n",
    "                'title': field.title,\n",
    "                'linkout': field.linkout\n",
    "            })\n",
    "            print(f\"✓ [{field.name}]\\t{field.title}\")\n",
    "        except:\n",
    "            # If exact match fails, search for all instances using regex\n",
    "            # Pattern matches: p50$ (exact) OR p50_i0, p50_i1, p50_i2, etc.\n",
    "            pattern = f\"^{field_name}$|^{field_name}_i\\\\d+$|^{field_name}_i\\\\d+_a\\\\d+$\"\n",
    "            instance_fields = list(pheno.find_fields(name_regex=pattern))\n",
    "            \n",
    "            if instance_fields:\n",
    "                for field in instance_fields:\n",
    "                    verified_fields.append({\n",
    "                        'name': field.name,\n",
    "                        'title': field.title,\n",
    "                        'linkout': field.linkout\n",
    "                    })\n",
    "                    print(f\"✓ [{field.name}]\\t{field.title}\")\n",
    "                print(f\"  → Found {len(instance_fields)} instances for {field_name}\")\n",
    "            else:\n",
    "                raise Exception(f\"No instances found\")\n",
    "                \n",
    "    except Exception as e:\n",
    "        missing_fields.append(field_name)\n",
    "        print(f\"✗ [{field_name}]\\tNot found: {str(e)}\")\n",
    "\n",
    "print(f\"\\n\\nSummary:\")\n",
    "print(f\"Found: {len(verified_fields)} fields (including all instances)\")\n",
    "print(f\"Missing: {len(missing_fields)} base fields\")\n",
    "if missing_fields:\n",
    "    print(f\"\\nMissing fields: {missing_fields}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save field names to JSON file\n",
    "\n",
    "Save the verified field names to a JSON file in the same format as the original field names.json file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# It is read-only file system\n",
    "\n",
    "# # Save field names to JSON file\n",
    "# output_file = '/mnt/project/extracted_field_names.json'\n",
    "\n",
    "# # Save as list (one field per line like the original)\n",
    "# with open(output_file, 'w') as f:\n",
    "#     json.dump([f['name'] for f in verified_fields], f, indent=0)\n",
    "\n",
    "# print(f\"Field names saved to: {output_file}\")\n",
    "# print(f\"Total fields saved: {len(verified_fields)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Data in Batches and Save\n",
    "\n",
    "This cell performs batch extraction to handle large numbers of fields efficiently.\n",
    "\n",
    "**IMPORTANT**: Uses **Direct Spark Save** to avoid Spark driver memory errors.\n",
    "\n",
    "**Features:**\n",
    "- Processes fields in batches (default: 100 fields per batch)\n",
    "- Saves directly from Spark DataFrame to CSV (avoids `toPandas()` memory issues)\n",
    "- Saves each batch as a separate CSV file (`ukb_batch_01_of_XX.csv`, etc.)\n",
    "- Automatically loads and merges all batches into a single pandas DataFrame\n",
    "- Includes memory cleanup after processing\n",
    "\n",
    "**Configuration:**\n",
    "- `BATCH_SIZE`: Number of fields per batch (default: 100, reduce if still getting errors)\n",
    "- `USE_SPARK_SAVE`: Whether to save directly from Spark (recommended: `True`)\n",
    "\n",
    "**Why Direct Spark Save?**\n",
    "When converting large Spark DataFrames to pandas using `toPandas()`, you may encounter:\n",
    "```\n",
    "Py4JJavaError: Total size of serialized results is bigger than spark.driver.maxResultSize (1024.0 MiB)\n",
    "```\n",
    "\n",
    "By setting `USE_SPARK_SAVE=True`, data is saved directly from Spark to CSV files, bypassing this limitation.\n",
    "\n",
    "**Benefits:**\n",
    "- Prevents Spark driver memory errors\n",
    "- Progress is saved incrementally (if extraction fails, you don't lose everything)\n",
    "- Each batch file can be used independently if needed\n",
    "- Can process any number of fields without hitting memory limits\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieve data from the table\n",
    "\n",
    "The following code selects the `participant` table.\n",
    "Then we can define which field we are interested in using the `find_field` function.\n",
    "\n",
    "There are three main ways to identify the field of interest:\n",
    "\n",
    "- With `name` argument: here we give field ID. We can construct field ID used by the `dxdata` package from the field ID defined by UKB Showcase. The numeric showcase ID is translated to the Spark DB column name by adding the letter `p` at the beginning: e.g. *Standing height* showcase id is `50`, so Spark ID would be `p50`. Usually, fields have multiple instances. In such case, we add the `_i` suffix followed by instance number, e.g. *Standing height | Instance 0* will be `p50_i0`\n",
    "- With `title` argument: here we define the field by full title, followed by ` | Instance` suffix, e.g. `Age at recruitment` or `Standing height | Instance 0`\n",
    "- With `title_regex` argument: here we define the field by [regular expression](https://docs.python.org/3/howto/regex.html) matching the part of the title. We can use a keyword here, e.g. `.*height.*` will return all columns with the word *height* in the title."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional: Reload and Merge Saved Batch Files\n",
    "\n",
    "If you've already run the batch extraction and saved the intermediate batch files, you can use this cell to reload and merge them without re-extracting from UK Biobank.\n",
    "\n",
    "**Usage:**\n",
    "```python\n",
    "# Uncomment to reload all batch files matching the pattern\n",
    "df = reload_and_merge_batches()\n",
    "\n",
    "# Or specify a custom pattern\n",
    "df = reload_and_merge_batches(batch_pattern='my_custom_batch_*.csv')\n",
    "```\n",
    "\n",
    "This is useful when:\n",
    "- You need to restart your notebook session\n",
    "- You want to work with the data without re-running the extraction\n",
    "- You're experimenting with different processing approaches\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [],
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracting data for 399 fields...\n",
      "Data extracted: 501936 participants, 399 fields\n",
      "\n",
      "Converting to pandas DataFrame...\n",
      "Full dataset shape: (501936, 399)\n",
      "\n",
      "First few rows:\n",
      "       eid  p1558_i0  p1558_i1  p1558_i2  p1558_i3  p1787_i0  p1787_i1  \\\n",
      "0  1000020       2.0       NaN       NaN       NaN       0.0       NaN   \n",
      "1  1000053       2.0       NaN       NaN       NaN       0.0       NaN   \n",
      "2  1000171       1.0       NaN       NaN       NaN      -1.0       NaN   \n",
      "3  1000186       5.0       NaN       NaN       NaN       0.0       NaN   \n",
      "4  1000199       5.0       NaN       NaN       NaN       1.0       NaN   \n",
      "\n",
      "   p1787_i2  p20002_i0_a0  p20002_i0_a1  ...  p6138_i2  p6138_i3  p6152_i0  \\\n",
      "0       NaN        1371.0        1473.0  ...      None      None      [-7]   \n",
      "1       NaN        1065.0        1458.0  ...      None      None      [-7]   \n",
      "2       NaN           NaN           NaN  ...      None      None      [-7]   \n",
      "3       NaN        1309.0        1265.0  ...      None      None      [-7]   \n",
      "4       NaN        1465.0        1353.0  ...      None      None       [9]   \n",
      "\n",
      "   p6152_i1  p6152_i2  p6152_i3  p738_i0  p738_i1  p738_i2  p738_i3  \n",
      "0      None      None      None      2.0      NaN      NaN      NaN  \n",
      "1      None      None      None      3.0      NaN      NaN      NaN  \n",
      "2      None      None      None      4.0      NaN      NaN      NaN  \n",
      "3      None      None      None      2.0      NaN      NaN      NaN  \n",
      "4      None      None      None      1.0      NaN      NaN      NaN  \n",
      "\n",
      "[5 rows x 399 columns]\n"
     ]
    }
   ],
   "source": [
    "# Get field objects for extraction\n",
    "field_objects = []\n",
    "for field_info in verified_fields:\n",
    "    try:\n",
    "        field = pheno.find_field(name=field_info['name'])\n",
    "        field_objects.append(field)\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading field {field_info['name']}: {e}\")\n",
    "\n",
    "# Add participant ID - this will be included in EVERY batch\n",
    "eid_field = pheno.find_field(name=\"eid\")\n",
    "\n",
    "# Ensure we have ICD-10 diagnosis fields for filtering\n",
    "# These fields contain the diagnosis codes\n",
    "icd10_fields = ['p41202', 'p41204', 'p41270']\n",
    "icd10_field_objects = []\n",
    "for icd_field in icd10_fields:\n",
    "    try:\n",
    "        # Check if already in field_objects\n",
    "        if not any(f.name == icd_field for f in field_objects):\n",
    "            field = pheno.find_field(name=icd_field)\n",
    "            icd10_field_objects.append(field)\n",
    "            print(f\"Added ICD-10 diagnosis field: {icd_field}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Could not add {icd_field}: {e}\")\n",
    "\n",
    "# Append ICD-10 fields to field_objects\n",
    "field_objects.extend(icd10_field_objects)\n",
    "\n",
    "print(f\"\\nTotal fields to extract: {len(field_objects)} (excluding eid)\")\n",
    "\n",
    "# ============================================================\n",
    "# BATCH PROCESSING CONFIGURATION\n",
    "# ============================================================\n",
    "BATCH_SIZE = 100  # Reduced to 100 fields per batch to avoid Spark memory errors\n",
    "USE_SPARK_SAVE = True  # Save directly from Spark (avoids driver maxResultSize error)\n",
    "\n",
    "# Split fields into batches\n",
    "total_batches = (len(field_objects) + BATCH_SIZE - 1) // BATCH_SIZE\n",
    "print(f\"Processing in {total_batches} batch(es) of up to {BATCH_SIZE} fields each\")\n",
    "print(f\"Save method: {'Direct Spark save (recommended)' if USE_SPARK_SAVE else 'Pandas conversion'}\")\n",
    "\n",
    "batch_files = []\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "for batch_idx in range(total_batches):\n",
    "    start_idx = batch_idx * BATCH_SIZE\n",
    "    end_idx = min((batch_idx + 1) * BATCH_SIZE, len(field_objects))\n",
    "    \n",
    "    # Get fields for this batch\n",
    "    batch_fields = [eid_field] + field_objects[start_idx:end_idx]\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"BATCH {batch_idx + 1}/{total_batches}\")\n",
    "    print(f\"Processing fields {start_idx + 1} to {end_idx} ({len(batch_fields)-1} fields + eid)\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    # Retrieve data for this batch (returns PySpark DataFrame)\n",
    "    print(f\"Extracting data from UK Biobank...\")\n",
    "    spark_df_batch = pheno.retrieve_fields(fields=batch_fields, engine=engine)\n",
    "    \n",
    "    # Get row count using PySpark method\n",
    "    row_count = spark_df_batch.count()\n",
    "    col_count = len(spark_df_batch.columns)\n",
    "    print(f\"Extracted: {row_count} participants, {col_count} fields\")\n",
    "    \n",
    "    # Save batch\n",
    "    batch_filename = f'ukb_batch_{batch_idx+1:02d}_of_{total_batches:02d}.csv'\n",
    "    \n",
    "    if USE_SPARK_SAVE:\n",
    "        # Save directly from Spark to avoid memory issues\n",
    "        print(f\"Saving directly from Spark DataFrame (avoids memory errors)...\")\n",
    "        \n",
    "        # Save as CSV using Spark (creates a directory with part files)\n",
    "        temp_dir = f'temp_batch_{batch_idx+1:02d}'\n",
    "        spark_df_batch.coalesce(1).write.mode('overwrite').option('header', 'true').csv(temp_dir)\n",
    "        \n",
    "        # Move the single CSV file from the temp directory to the final location\n",
    "        # Find the part file (usually part-00000*.csv)\n",
    "        part_files = [f for f in os.listdir(temp_dir) if f.startswith('part-') and f.endswith('.csv')]\n",
    "        if part_files:\n",
    "            source_file = os.path.join(temp_dir, part_files[0])\n",
    "            shutil.move(source_file, batch_filename)\n",
    "            file_size = os.path.getsize(batch_filename) / (1024**2)  # Size in MB\n",
    "            print(f\"✓ Saved batch to: {batch_filename} ({file_size:.2f} MB)\")\n",
    "        else:\n",
    "            print(f\"Warning: No part file found in {temp_dir}\")\n",
    "        \n",
    "        # Clean up temp directory\n",
    "        shutil.rmtree(temp_dir, ignore_errors=True)\n",
    "        \n",
    "    else:\n",
    "        # Alternative: Convert to pandas first (may fail with large batches)\n",
    "        try:\n",
    "            print(f\"Converting to pandas DataFrame...\")\n",
    "            df_batch = spark_df_batch.toPandas()\n",
    "            print(f\"Batch shape: {df_batch.shape}\")\n",
    "            \n",
    "            df_batch.to_csv(batch_filename, index=False)\n",
    "            file_size = os.path.getsize(batch_filename) / (1024**2)  # Size in MB\n",
    "            print(f\"✓ Saved batch to: {batch_filename} ({file_size:.2f} MB)\")\n",
    "        except Exception as e:\n",
    "            print(f\"ERROR: Failed to convert batch {batch_idx+1} to pandas: {e}\")\n",
    "            print(f\"Consider reducing BATCH_SIZE or setting USE_SPARK_SAVE=True\")\n",
    "            raise\n",
    "    \n",
    "    batch_files.append(batch_filename)\n",
    "    print(f\"✓ Batch {batch_idx + 1} completed\")\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"ALL BATCHES SAVED\")\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"Saved {len(batch_files)} batch files:\")\n",
    "total_size = 0\n",
    "for bf in batch_files:\n",
    "    if os.path.exists(bf):\n",
    "        file_size = os.path.getsize(bf) / (1024**2)  # Size in MB\n",
    "        total_size += file_size\n",
    "        print(f\"  ✓ {bf} ({file_size:.2f} MB)\")\n",
    "print(f\"Total size: {total_size:.2f} MB\")\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"LOADING AND MERGING ALL BATCHES\")\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"Note: Loading saved CSV files into pandas for merging...\")\n",
    "\n",
    "# Load all batch CSVs into pandas DataFrames\n",
    "import pandas as pd\n",
    "batch_dataframes = []\n",
    "\n",
    "for i, batch_file in enumerate(batch_files, start=1):\n",
    "    print(f\"Loading batch {i}/{len(batch_files)}: {batch_file}\")\n",
    "    df_batch = pd.read_csv(batch_file)\n",
    "    print(f\"  Shape: {df_batch.shape}\")\n",
    "    batch_dataframes.append(df_batch)\n",
    "\n",
    "# Merge all batches on 'eid' (participant ID)\n",
    "print(f\"\\nMerging batches...\")\n",
    "df = batch_dataframes[0]\n",
    "for i, batch_df in enumerate(batch_dataframes[1:], start=2):\n",
    "    print(f\"  Merging batch {i}...\")\n",
    "    # Drop 'eid' from subsequent batches before merging to avoid duplication\n",
    "    batch_df_no_eid = batch_df.drop(columns=['eid'])\n",
    "    df = df.join(batch_df_no_eid)\n",
    "\n",
    "print(f\"\\n✓ All batches merged successfully!\")\n",
    "print(f\"Full dataset shape: {df.shape}\")\n",
    "print(f\"Total participants: {df.shape[0]}\")\n",
    "print(f\"Total fields: {df.shape[1]} (including eid)\")\n",
    "\n",
    "# Display first few rows\n",
    "print(\"\\nFirst few rows:\")\n",
    "print(df.head())\n",
    "\n",
    "# Clean up memory\n",
    "del batch_dataframes\n",
    "import gc\n",
    "gc.collect()\n",
    "\n",
    "print(f\"\\n✓ Memory cleaned up\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": [],
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>eid</th>\n",
       "      <th>p1558_i0</th>\n",
       "      <th>p1558_i1</th>\n",
       "      <th>p1558_i2</th>\n",
       "      <th>p1558_i3</th>\n",
       "      <th>p1787_i0</th>\n",
       "      <th>p1787_i1</th>\n",
       "      <th>p1787_i2</th>\n",
       "      <th>p20116_i0</th>\n",
       "      <th>p20116_i1</th>\n",
       "      <th>...</th>\n",
       "      <th>p6138_i2</th>\n",
       "      <th>p6138_i3</th>\n",
       "      <th>p6152_i0</th>\n",
       "      <th>p6152_i1</th>\n",
       "      <th>p6152_i2</th>\n",
       "      <th>p6152_i3</th>\n",
       "      <th>p738_i0</th>\n",
       "      <th>p738_i1</th>\n",
       "      <th>p738_i2</th>\n",
       "      <th>p738_i3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1000020</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[-7]</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1000053</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[-7]</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1000171</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[-7]</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1000186</td>\n",
       "      <td>5.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[-7]</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1000199</td>\n",
       "      <td>5.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[9]</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 209 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       eid  p1558_i0  p1558_i1  p1558_i2  p1558_i3  p1787_i0  p1787_i1  \\\n",
       "0  1000020       2.0       NaN       NaN       NaN       0.0       NaN   \n",
       "1  1000053       2.0       NaN       NaN       NaN       0.0       NaN   \n",
       "2  1000171       1.0       NaN       NaN       NaN      -1.0       NaN   \n",
       "3  1000186       5.0       NaN       NaN       NaN       0.0       NaN   \n",
       "4  1000199       5.0       NaN       NaN       NaN       1.0       NaN   \n",
       "\n",
       "   p1787_i2  p20116_i0  p20116_i1  ...  p6138_i2  p6138_i3  p6152_i0 p6152_i1  \\\n",
       "0       NaN        0.0        NaN  ...      None      None      [-7]     None   \n",
       "1       NaN        0.0        NaN  ...      None      None      [-7]     None   \n",
       "2       NaN        1.0        NaN  ...      None      None      [-7]     None   \n",
       "3       NaN        0.0        NaN  ...      None      None      [-7]     None   \n",
       "4       NaN        0.0        NaN  ...      None      None       [9]     None   \n",
       "\n",
       "  p6152_i2 p6152_i3 p738_i0 p738_i1 p738_i2  p738_i3  \n",
       "0     None     None     2.0     NaN     NaN      NaN  \n",
       "1     None     None     3.0     NaN     NaN      NaN  \n",
       "2     None     None     4.0     NaN     NaN      NaN  \n",
       "3     None     None     2.0     NaN     NaN      NaN  \n",
       "4     None     None     1.0     NaN     NaN      NaN  \n",
       "\n",
       "[5 rows x 209 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter for Respiratory Disease Cohort\n",
    "\n",
    "We will filter participants to include only those with respiratory disease diagnoses based on ICD-10 codes:\n",
    "- **J09-J98**: Diseases of the respiratory system\n",
    "- **I26-I27**: Pulmonary heart disease and diseases of pulmonary circulation\n",
    "\n",
    "This filtering will be applied after data extraction using the diagnosis fields (p41202 \"Diagnoses - main ICD10\", p41204 \"Diagnoses - secondary ICD10\", p41270 \"Diagnoses ICD10\").\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "tags": [],
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering for respiratory disease patients...\n",
      "\n",
      "Cohort filtering results:\n",
      "Original dataset: 501936 participants\n",
      "Respiratory disease cohort: 133842 participants (26.67%)\n",
      "\n",
      "Example diagnoses from filtered cohort (first 5 participants):\n",
      "  Participant 1000496: J181, J90, J981, J181, J90, J981\n",
      "  Participant 1000517: J151, J181, J189, J90, J439, J440, J9690, J151, J181, J189\n",
      "  Participant 1001018: I269, I269, J439, J981, J984, I269, J439, J981, J984\n",
      "  Participant 1001080: J189, J189, J189\n",
      "  Participant 1001128: J13, J459, J13, J459\n",
      "\n",
      "Filtered respiratory cohort saved to: ukb_respiratory_cohort.csv\n",
      "Full dataset saved to: ukb_full_data.csv\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def has_respiratory_diagnosis(row):\n",
    "    \"\"\"\n",
    "    Check if a participant has respiratory disease diagnosis based on ICD-10 codes:\n",
    "    - J09-J98: Diseases of the respiratory system\n",
    "    - I26-I27: Pulmonary heart disease and diseases of pulmonary circulation\n",
    "    \n",
    "    Note: ICD-10 codes may be stored without decimal points (e.g., J181 = J18.1)\n",
    "    \"\"\"\n",
    "    # Collect all diagnosis codes from available ICD-10 fields\n",
    "    all_diagnoses = []\n",
    "    \n",
    "    for field in ['p41202', 'p41204', 'p41270']:\n",
    "        if field in row.index and row[field] is not None:\n",
    "            # Handle both list and single string values\n",
    "            if isinstance(row[field], list):\n",
    "                all_diagnoses.extend(row[field])\n",
    "            else:\n",
    "                all_diagnoses.append(str(row[field]))\n",
    "    \n",
    "    # Check each diagnosis code\n",
    "    for code in all_diagnoses:\n",
    "        if code and isinstance(code, str):\n",
    "            # Remove any whitespace and convert to uppercase\n",
    "            code = code.strip().upper()\n",
    "            \n",
    "            # Check for J09-J98 (respiratory diseases)\n",
    "            # ICD-10 format: Letter + 2-3 digits (category) + optional subcategory\n",
    "            # Examples: J18.1 (stored as J181), J96.90 (stored as J9690)\n",
    "            if code.startswith('J'):\n",
    "                # Extract the category code (first 2 digits after 'J')\n",
    "                # For J codes, category is always 2 digits: J00-J99\n",
    "                match = re.match(r'J(\\d{2})', code)\n",
    "                if match:\n",
    "                    category = int(match.group(1))\n",
    "                    if 9 <= category <= 98:\n",
    "                        return True\n",
    "            \n",
    "            # Check for I26-I27 (pulmonary heart disease)\n",
    "            # For I codes, category is also 2 digits: I00-I99\n",
    "            elif code.startswith('I'):\n",
    "                match = re.match(r'I(\\d{2})', code)\n",
    "                if match:\n",
    "                    category = int(match.group(1))\n",
    "                    if 26 <= category <= 27:\n",
    "                        return True\n",
    "    \n",
    "    return False\n",
    "\n",
    "# Apply filter to get respiratory disease cohort\n",
    "print(\"Filtering for respiratory disease patients...\")\n",
    "df['has_respiratory_disease'] = df.apply(has_respiratory_diagnosis, axis=1)\n",
    "\n",
    "respiratory_df = df[df['has_respiratory_disease'] == True].copy()\n",
    "respiratory_df = respiratory_df.drop(columns=['has_respiratory_disease'])\n",
    "\n",
    "print(f\"\\nCohort filtering results:\")\n",
    "print(f\"Original dataset: {len(df)} participants\")\n",
    "print(f\"Respiratory disease cohort: {len(respiratory_df)} participants ({len(respiratory_df)/len(df)*100:.2f}%)\")\n",
    "\n",
    "# Show some diagnosis examples\n",
    "print(\"\\nExample diagnoses from filtered cohort (first 5 participants):\")\n",
    "for idx, row in respiratory_df.head().iterrows():\n",
    "    diagnoses = []\n",
    "    for field in ['p41202', 'p41204', 'p41270']:\n",
    "        if field in row.index and row[field] is not None:\n",
    "            if isinstance(row[field], list):\n",
    "                diagnoses.extend(row[field])\n",
    "            else:\n",
    "                diagnoses.append(str(row[field]))\n",
    "    \n",
    "    # Filter for respiratory codes only using correct parsing\n",
    "    resp_codes = []\n",
    "    for d in diagnoses:\n",
    "        if d and isinstance(d, str):\n",
    "            d_clean = d.strip().upper()\n",
    "            # Check J09-J98\n",
    "            if d_clean.startswith('J'):\n",
    "                match = re.match(r'J(\\d{2})', d_clean)\n",
    "                if match and 9 <= int(match.group(1)) <= 98:\n",
    "                    resp_codes.append(d)\n",
    "            # Check I26-I27\n",
    "            elif d_clean.startswith('I'):\n",
    "                match = re.match(r'I(\\d{2})', d_clean)\n",
    "                if match and 26 <= int(match.group(1)) <= 27:\n",
    "                    resp_codes.append(d)\n",
    "    \n",
    "    print(f\"  Participant {row['eid']}: {', '.join(resp_codes[:10])}\")  # Show first 10 codes\n",
    "\n",
    "# Save filtered cohort to CSV\n",
    "output_csv = 'ukb_respiratory_cohort.csv'\n",
    "respiratory_df.to_csv(output_csv, index=False)\n",
    "print(f\"\\nFiltered respiratory cohort saved to: {output_csv}\")\n",
    "\n",
    "# Also save full dataset (unfiltered) for comparison\n",
    "output_csv_full = 'ukb_full_data.csv'\n",
    "df.to_csv(output_csv_full, index=False)\n",
    "print(f\"Full dataset saved to: {output_csv_full}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze ICD-10 Code Distribution in Respiratory Cohort\n",
    "\n",
    "Let's examine the distribution of respiratory ICD-10 codes to understand the types of respiratory diseases in our cohort.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [],
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total respiratory diagnoses recorded: 590935\n",
      "Unique respiratory diagnosis codes: 211\n",
      "\n",
      "Top 20 most common respiratory diagnoses:\n",
      "------------------------------------------------------------\n",
      "J459     : 101651 cases (75.95% of cohort)\n",
      "J449     :  43193 cases (32.27% of cohort)\n",
      "J181     :  42408 cases (31.69% of cohort)\n",
      "J22      :  40329 cases (30.13% of cohort)\n",
      "J90      :  38496 cases (28.76% of cohort)\n",
      "J189     :  29217 cases (21.83% of cohort)\n",
      "I269     :  20890 cases (15.61% of cohort)\n",
      "J981     :  20739 cases (15.50% of cohort)\n",
      "J440     :  18262 cases (13.64% of cohort)\n",
      "J47      :  14351 cases (10.72% of cohort)\n",
      "J342     :  11746 cases ( 8.78% of cohort)\n",
      "J439     :  11191 cases ( 8.36% of cohort)\n",
      "J301     :   9337 cases ( 6.98% of cohort)\n",
      "J984     :   8852 cases ( 6.61% of cohort)\n",
      "J348     :   8633 cases ( 6.45% of cohort)\n",
      "J690     :   8080 cases ( 6.04% of cohort)\n",
      "J841     :   7902 cases ( 5.90% of cohort)\n",
      "J128     :   6951 cases ( 5.19% of cohort)\n",
      "J9690    :   6903 cases ( 5.16% of cohort)\n",
      "J329     :   6810 cases ( 5.09% of cohort)\n",
      "\n",
      "\n",
      "Breakdown by ICD-10 category:\n",
      "J codes (Respiratory system diseases): 561430 diagnoses across 204 unique codes\n",
      "I26-I27 codes (Pulmonary heart disease): 29505 diagnoses across 7 unique codes\n",
      "\n",
      "\n",
      "Code statistics saved to: icd10_code_statistics.json\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Collect all respiratory ICD-10 codes from the cohort\n",
    "all_resp_codes = []\n",
    "\n",
    "for idx, row in respiratory_df.iterrows():\n",
    "    for field in ['p41202', 'p41204', 'p41270']:\n",
    "        if field in row.index and row[field] is not None:\n",
    "            if isinstance(row[field], list):\n",
    "                codes = row[field]\n",
    "            else:\n",
    "                codes = [str(row[field])]\n",
    "            \n",
    "            # Filter for respiratory codes (J09-J98, I26-I27)\n",
    "            for code in codes:\n",
    "                if code and isinstance(code, str):\n",
    "                    code = code.strip().upper()\n",
    "                    # Check if it's a respiratory code\n",
    "                    # Use correct ICD-10 parsing: extract first 2 digits (category)\n",
    "                    if code.startswith('J'):\n",
    "                        match = re.match(r'J(\\d{2})', code)\n",
    "                        if match:\n",
    "                            category = int(match.group(1))\n",
    "                            if 9 <= category <= 98:\n",
    "                                all_resp_codes.append(code)\n",
    "                    elif code.startswith('I'):\n",
    "                        match = re.match(r'I(\\d{2})', code)\n",
    "                        if match:\n",
    "                            category = int(match.group(1))\n",
    "                            if 26 <= category <= 27:\n",
    "                                all_resp_codes.append(code)\n",
    "\n",
    "# Count frequency of each code\n",
    "code_counts = Counter(all_resp_codes)\n",
    "\n",
    "print(f\"Total respiratory diagnoses recorded: {len(all_resp_codes)}\")\n",
    "print(f\"Unique respiratory diagnosis codes: {len(code_counts)}\")\n",
    "print(f\"\\nTop 20 most common respiratory diagnoses:\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for code, count in code_counts.most_common(20):\n",
    "    percentage = (count / len(respiratory_df)) * 100\n",
    "    print(f\"{code:8s} : {count:6d} cases ({percentage:5.2f}% of cohort)\")\n",
    "\n",
    "# Breakdown by major category\n",
    "j_codes = {k: v for k, v in code_counts.items() if k.startswith('J')}\n",
    "i_codes = {k: v for k, v in code_counts.items() if k.startswith('I')}\n",
    "\n",
    "print(f\"\\n\\nBreakdown by ICD-10 category:\")\n",
    "print(f\"J codes (Respiratory system diseases): {sum(j_codes.values())} diagnoses across {len(j_codes)} unique codes\")\n",
    "print(f\"I26-I27 codes (Pulmonary heart disease): {sum(i_codes.values())} diagnoses across {len(i_codes)} unique codes\")\n",
    "\n",
    "# Save code statistics to file\n",
    "import json\n",
    "code_stats = {\n",
    "    'total_participants': len(respiratory_df),\n",
    "    'total_diagnoses': len(all_resp_codes),\n",
    "    'unique_codes': len(code_counts),\n",
    "    'top_20_codes': [{'code': code, 'count': count, 'percentage': round((count/len(respiratory_df))*100, 2)} \n",
    "                     for code, count in code_counts.most_common(20)],\n",
    "    'category_breakdown': {\n",
    "        'J_codes': {'count': sum(j_codes.values()), 'unique': len(j_codes)},\n",
    "        'I_codes': {'count': sum(i_codes.values()), 'unique': len(i_codes)}\n",
    "    }\n",
    "}\n",
    "\n",
    "stats_file = 'icd10_code_statistics.json'\n",
    "with open(stats_file, 'w') as f:\n",
    "    json.dump(code_stats, f, indent=2)\n",
    "    \n",
    "print(f\"\\n\\nCode statistics saved to: {stats_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "be0617d24f23f3f0ff0f78cfff875dd0cc8ce9ddccca39efd47dcbfb80ba815b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
